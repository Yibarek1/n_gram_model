{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwZWVyudacyD"
      },
      "source": [
        "# Dataset Creation for N-gram Language Model\n",
        "\n",
        "**from** Yibarek Tadesse\n",
        "\n",
        "**Course:** GenAI for Software Development (CSCI 455/555)  \n",
        "**Date:** February 2026  \n",
        "**Instructor:** Dr. Antonio Mastropaolo  \n",
        "**TA:** Md. Zahidul Haque Alvi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us120SLFacyE"
      },
      "source": [
        "---\n",
        "## Setup & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-U33CEYUacyE",
        "outputId": "897d1290-d214-47fd-c623-ab477dc25051"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: javalang in c:\\users\\creep\\appdata\\roaming\\python\\python314\\site-packages (0.13.0)\n",
            "Requirement already satisfied: gitpython in c:\\users\\creep\\appdata\\roaming\\python\\python314\\site-packages (3.1.46)\n",
            "Requirement already satisfied: pandas in c:\\users\\creep\\appdata\\roaming\\python\\python314\\site-packages (3.0.1)\n",
            "Requirement already satisfied: six in c:\\users\\creep\\appdata\\roaming\\python\\python314\\site-packages (from javalang) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\creep\\appdata\\roaming\\python\\python314\\site-packages (from gitpython) (4.0.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\creep\\appdata\\roaming\\python\\python314\\site-packages (from gitdb<5,>=4.0.1->gitpython) (5.0.2)\n",
            "Requirement already satisfied: numpy>=2.3.3 in c:\\users\\creep\\appdata\\roaming\\python\\python314\\site-packages (from pandas) (2.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\creep\\appdata\\roaming\\python\\python314\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata in c:\\users\\creep\\appdata\\roaming\\python\\python314\\site-packages (from pandas) (2025.3)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 26.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install javalang gitpython pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eI8tyF_SacyF",
        "outputId": "213a59d5-7b8d-4931-d03e-28fd095a618d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setup complete!\n",
            "Clone directory: C:\\Users\\creep\\Downloads\\n_gram_model\\dataset\\java_repos\n",
            "Output directory: C:\\Users\\creep\\Downloads\\n_gram_model\\dataset\\ngram_dataset\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import json\n",
        "import random\n",
        "import shutil\n",
        "import stat\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "import javalang\n",
        "from javalang.tokenizer import tokenize\n",
        "\n",
        "BASE_DIR = Path(\".\") / \"dataset\"\n",
        "CLONE_DIR = BASE_DIR / \"java_repos\"\n",
        "OUTPUT_DIR = BASE_DIR / \"ngram_dataset\"\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "CLASSES_PER_REPO = 10   # Java files to sample per repo\n",
        "MIN_TOKENS = 10         # Minimum tokens per method\n",
        "\n",
        "VAL_SIZE = 1000\n",
        "TEST_SIZE = 1000\n",
        "T1_CAP = 15000\n",
        "T2_CAP = 25000\n",
        "T3_CAP = 35000\n",
        "\n",
        "# Create directories if they do not exist\n",
        "\n",
        "CLONE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Setup complete!\")\n",
        "print(f\"Clone directory: {CLONE_DIR.resolve()}\")\n",
        "print(f\"Output directory: {OUTPUT_DIR.resolve()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3UAsk7qacyF"
      },
      "source": [
        "---\n",
        "## Fetch Repository List\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "1OpS-ZHDacyG",
        "outputId": "0a00b017-f4a0-4ea7-81e5-390bc278c6d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching top Java repositories from GitHub...\n",
            "\n",
            "Fetched 500 repositories\n",
            "\n",
            "Top 10 repos by stars:\n",
            "                                full_name  \\\n",
            "0           iluwatar/java-design-patterns   \n",
            "1                         macrozheng/mall   \n",
            "2             spring-projects/spring-boot   \n",
            "3                     doocs/advanced-java   \n",
            "4                   elastic/elasticsearch   \n",
            "..                                    ...   \n",
            "495                     WhatsApp/stickers   \n",
            "496              bytedeco/javacpp-presets   \n",
            "497                       assertj/assertj   \n",
            "498  sqlcipher/android-database-sqlcipher   \n",
            "499                     apache/cloudstack   \n",
            "\n",
            "                                             clone_url  stars  size_kb  \\\n",
            "0    https://github.com/iluwatar/java-design-patter...  93757    47229   \n",
            "1               https://github.com/macrozheng/mall.git  82927    58726   \n",
            "2    https://github.com/spring-projects/spring-boot...  80050   211658   \n",
            "3           https://github.com/doocs/advanced-java.git  78839    26635   \n",
            "4         https://github.com/elastic/elasticsearch.git  76164  1555077   \n",
            "..                                                 ...    ...      ...   \n",
            "495           https://github.com/WhatsApp/stickers.git   2864    10193   \n",
            "496    https://github.com/bytedeco/javacpp-presets.git   2830   779282   \n",
            "497             https://github.com/assertj/assertj.git   2810    55711   \n",
            "498  https://github.com/sqlcipher/android-database-...   2808    74545   \n",
            "499           https://github.com/apache/cloudstack.git   2799   653296   \n",
            "\n",
            "              last_pushed                                        description  \n",
            "0    2026-02-10T01:04:11Z                Design patterns implemented in Java  \n",
            "1    2026-02-02T09:25:28Z  mallé¡¹ç›®æ˜¯ä¸€å¥—ç”µå•†ç³»ç»Ÿï¼ŒåŒ…æ‹¬å‰å°å•†åŸŽç³»ç»ŸåŠåŽå°ç®¡ç†ç³»ç»Ÿï¼ŒåŸºäºŽSpring Boot+My...  \n",
            "2    2026-02-19T16:02:39Z  Spring Boot helps you to create Spring-powered...  \n",
            "3    2026-01-21T23:54:19Z  ðŸ˜® Core Interview Questions & Answers For Exper...  \n",
            "4    2026-02-20T08:59:59Z  Free and Open Source, Distributed, RESTful Sea...  \n",
            "..                    ...                                                ...  \n",
            "495  2025-11-07T13:54:43Z  This repository contains the iOS and Android s...  \n",
            "496  2026-02-20T09:19:34Z  The missing Java distribution of native C++ li...  \n",
            "497  2026-02-19T21:42:58Z     Fluent testing assertions for Java and the JVM  \n",
            "498  2025-09-17T14:14:01Z              Android SQLite API based on SQLCipher  \n",
            "499  2026-02-20T04:58:14Z  Apache CloudStack is an opensource Infrastruct...  \n",
            "\n",
            "[500 rows x 6 columns]\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "def fetch_top_java_repos(num_repos=500, per_page=100):\n",
        "    \"\"\"\n",
        "    Fetch top-starred Java repositories from GitHub API.\n",
        "    Filters applied:\n",
        "    - language:java (Fixed language requirement)\n",
        "    - stars:>1000 (Popularity)\n",
        "    - size:>10000 (Size > 10MB)\n",
        "    - pushed:>2025-01-01 (Recent activity)\n",
        "    - archived:false (Exclusion rule)\n",
        "    - topic:leetcode/interview (Avoid impractical, isolated code)\n",
        "    - fork=False (Exclusion rule to avoid duplicate code)\n",
        "    \"\"\"\n",
        "    repos = []\n",
        "    page = 1\n",
        "\n",
        "    while len(repos) < num_repos:\n",
        "        url = \"https://api.github.com/search/repositories\"\n",
        "        \n",
        "        # Query string with defined criteria\n",
        "        params = {\n",
        "            \"q\": \"language:java stars:>1000 size:>10000 pushed:>2025-01-01 archived:false -topic:leetcode -topic:interview\",\n",
        "            \"sort\": \"stars\",\n",
        "            \"order\": \"desc\",\n",
        "            \"per_page\": per_page,\n",
        "            \"page\": page\n",
        "        }\n",
        "\n",
        "        response = requests.get(url, params=params)\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Error: {response.status_code}\")\n",
        "            break\n",
        "\n",
        "        data = response.json()\n",
        "        items = data.get(\"items\", [])\n",
        "\n",
        "        if not items:\n",
        "            break\n",
        "\n",
        "        for item in items:\n",
        "            # Explicitly exclude forks to prevent data duplication\n",
        "            if item.get(\"fork\", False):\n",
        "                continue\n",
        "\n",
        "            repos.append({\n",
        "                \"full_name\": item[\"full_name\"],\n",
        "                \"clone_url\": item[\"clone_url\"],\n",
        "                \"stars\": item[\"stargazers_count\"],\n",
        "                \"size_kb\": item[\"size\"],\n",
        "                \"last_pushed\": item[\"pushed_at\"],\n",
        "                \"description\": item.get(\"description\", \"\")\n",
        "            })\n",
        "\n",
        "        page += 1\n",
        "\n",
        "        if len(repos) >= num_repos:\n",
        "            break\n",
        "\n",
        "    return repos[:num_repos]\n",
        "\n",
        "# Fetch repositories\n",
        "print(\"Fetching top Java repositories from GitHub...\")\n",
        "repo_data = fetch_top_java_repos(num_repos=500)\n",
        "df_repos = pd.DataFrame(repo_data)\n",
        "\n",
        "print(f\"\\nFetched {len(df_repos)} repositories\")\n",
        "print(f\"\\nTop 10 repos by stars:\")\n",
        "print(df_repos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gl4uIgGmacyG",
        "outputId": "84c10df3-c611-4f4f-c99a-455e45197d0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning repositories ...\n",
            "\n",
            "[1/500] Cloning iluwatar/java-design-patterns... done\n",
            "[2/500] Cloning macrozheng/mall... done\n",
            "[3/500] Cloning spring-projects/spring-boot... done\n",
            "[4/500] Cloning doocs/advanced-java... done\n",
            "[5/500] Cloning elastic/elasticsearch... done\n",
            "[6/500] Cloning NationalSecurityAgency/ghidra... done\n",
            "[7/500] Cloning spring-projects/spring-framework... done\n",
            "[8/500] Cloning google/guava... done\n",
            "[9/500] Cloning dbeaver/dbeaver... done\n",
            "[10/500] Cloning ReactiveX/RxJava... done\n",
            "[11/500] Cloning skylot/jadx... done\n",
            "[12/500] Cloning jeecgboot/JeecgBoot... done\n",
            "[13/500] Cloning apache/dubbo... done\n",
            "[14/500] Cloning PhilJay/MPAndroidChart... done\n",
            "[15/500] Cloning halo-dev/halo... done\n",
            "[16/500] Cloning eugenp/tutorials... done\n",
            "[17/500] Cloning TeamNewPipe/NewPipe... done\n",
            "[18/500] Cloning alibaba/arthas... done\n",
            "[19/500] Cloning airbnb/lottie-android... done\n",
            "[20/500] Cloning YunaiV/ruoyi-vue-pro... done\n",
            "[21/500] Cloning bumptech/glide... done\n",
            "[22/500] Cloning netty/netty... done\n",
            "[23/500] Cloning SeleniumHQ/selenium... done\n",
            "[24/500] Cloning zxing/zxing... done\n",
            "[25/500] Cloning keycloak/keycloak... done\n",
            "[26/500] Cloning alibaba/nacos... done\n",
            "[27/500] Cloning binarywang/WxJava... done\n",
            "[28/500] Cloning apache/kafka... done\n",
            "[29/500] Cloning conductor-oss/conductor... done\n",
            "[30/500] Cloning chinabugotech/hutool... done\n",
            "[31/500] Cloning xuxueli/xxl-job... done\n",
            "[32/500] Cloning apolloconfig/apollo... done\n",
            "[33/500] Cloning alibaba/canal... done\n",
            "[34/500] Cloning alibaba/spring-cloud-alibaba... done\n",
            "[35/500] Cloning DrKLO/Telegram... done\n",
            "[36/500] Cloning yuliskov/SmartTube... done\n",
            "[37/500] Cloning alibaba/druid... done\n",
            "[38/500] Cloning Anuken/Mindustry... done\n",
            "[39/500] Cloning qiurunze123/miaosha... done\n",
            "[40/500] Cloning kestra-io/kestra... done\n",
            "[41/500] Cloning apache/incubator-seata... done\n",
            "[42/500] Cloning OpenAPITools/openapi-generator... done\n",
            "[43/500] Cloning apache/flink... done\n",
            "[44/500] Cloning CodePhiliaX/Chat2DB... done\n",
            "[45/500] Cloning bazelbuild/bazel... done\n",
            "[46/500] Cloning jenkinsci/jenkins... done\n",
            "[47/500] Cloning libgdx/libgdx... done\n",
            "[48/500] Cloning apache/skywalking... done\n",
            "[49/500] Cloning google/gson... done\n",
            "[50/500] Cloning redisson/redisson... done\n",
            "[51/500] Cloning iBotPeaches/Apktool... done\n",
            "[52/500] Cloning dataease/dataease... done\n",
            "[53/500] Cloning doocs/source-code-hunter... done\n",
            "[54/500] Cloning LSPosed/LSPosed... done\n",
            "[55/500] Cloning openjdk/jdk... done\n",
            "[56/500] Cloning apache/rocketmq... done\n",
            "[57/500] Cloning google/ExoPlayer... done\n",
            "[58/500] Cloning oracle/graal... done\n",
            "[59/500] Cloning CarGuo/GSYVideoPlayer... done\n",
            "[60/500] Cloning thingsboard/thingsboard... done\n",
            "[61/500] Cloning apache/shardingsphere... done\n",
            "[62/500] Cloning didi/DoKit... done\n",
            "[63/500] Cloning mybatis/mybatis-3... done\n",
            "[64/500] Cloning linlinjava/litemall... done\n",
            "[65/500] Cloning JetBrains/intellij-community... done\n",
            "[66/500] Cloning dianping/cat... done\n",
            "[67/500] Cloning antlr/antlr4... done\n",
            "[68/500] Cloning lionsoul2014/ip2region... done\n",
            "[69/500] Cloning YunaiV/yudao-cloud... done\n",
            "[70/500] Cloning williamfiset/Algorithms... done\n",
            "[71/500] Cloning Tencent/APIJSON... done\n",
            "[72/500] Cloning LMAX-Exchange/disruptor... done\n",
            "[73/500] Cloning ben-manes/caffeine... done\n",
            "[74/500] Cloning infinilabs/analysis-ik... done\n",
            "[75/500] Cloning openzipkin/zipkin... done\n",
            "[76/500] Cloning baomidou/mybatis-plus... done\n",
            "[77/500] Cloning material-components/material-components-android... done\n",
            "[78/500] Cloning facebook/fresco... done\n",
            "[79/500] Cloning alibaba/DataX... done\n",
            "[80/500] Cloning Grasscutters/Grasscutter... done\n",
            "[81/500] Cloning questdb/questdb... done\n",
            "[82/500] Cloning prestodb/presto... done\n",
            "[83/500] Cloning neo4j/neo4j... done\n",
            "[84/500] Cloning apache/hadoop... done\n",
            "[85/500] Cloning quarkusio/quarkus... done\n",
            "[86/500] Cloning mockito/mockito... done\n",
            "[87/500] Cloning Konloch/bytecode-viewer... done\n",
            "[88/500] Cloning apache/pulsar... done\n",
            "[89/500] Cloning zhisheng17/flink-learning... done\n",
            "[90/500] Cloning apache/doris... done\n",
            "[91/500] Cloning supertokens/supertokens-core... done\n",
            "[92/500] Cloning elastic/logstash... done\n",
            "[93/500] Cloning zaproxy/zaproxy... done\n",
            "[94/500] Cloning theonedev/onedev... done\n",
            "[95/500] Cloning eclipse-vertx/vert.x... done\n",
            "[96/500] Cloning cryptomator/cryptomator... done\n",
            "[97/500] Cloning arduino/Arduino... done\n",
            "[98/500] Cloning GoogleContainerTools/jib... done\n",
            "[99/500] Cloning deeplearning4j/deeplearning4j... done\n",
            "[100/500] Cloning apache/dolphinscheduler... done\n",
            "[101/500] Cloning languagetool-org/languagetool... done\n",
            "[102/500] Cloning kekingcn/kkFileView... done\n",
            "[103/500] Cloning apache/druid... done\n",
            "[104/500] Cloning janishar/mit-deep-learning-book-pdf... done\n",
            "[105/500] Cloning pinpoint-apm/pinpoint... done\n",
            "[106/500] Cloning xpipe-io/xpipe... done\n",
            "[107/500] Cloning projectlombok/lombok... done\n",
            "[108/500] Cloning metersphere/metersphere... done\n",
            "[109/500] Cloning macrozheng/mall-swarm... done\n",
            "[110/500] Cloning codecentric/spring-boot-admin... done\n",
            "[111/500] Cloning apache/zookeeper... done\n",
            "[112/500] Cloning google/guice... done\n",
            "[113/500] Cloning Netflix/eureka... done\n",
            "[114/500] Cloning plantuml/plantuml... done\n",
            "[115/500] Cloning trinodb/trino... done\n",
            "[116/500] Cloning debezium/debezium... done\n",
            "[117/500] Cloning opensearch-project/OpenSearch... done\n",
            "[118/500] Cloning microg/GmsCore... done\n",
            "[119/500] Cloning redis/jedis... done\n",
            "[120/500] Cloning LawnchairLauncher/lawnchair... done\n",
            "[121/500] Cloning Yalantis/uCrop... done\n",
            "[122/500] Cloning grpc/grpc-java... done\n",
            "[123/500] Cloning beemdevelopment/Aegis... done\n",
            "[124/500] Cloning PaperMC/Paper... done\n",
            "[125/500] Cloning OpenRefine/OpenRefine... done\n",
            "[126/500] Cloning code4craft/webmagic... done\n",
            "[127/500] Cloning datahub-project/datahub... done\n",
            "[128/500] Cloning newbee-ltd/newbee-mall... done\n",
            "[129/500] Cloning realm/realm-java... done\n",
            "[130/500] Cloning StarRocks/starrocks... done\n",
            "[131/500] Cloning jd-opensource/joyagent-jdgenie... done\n",
            "[132/500] Cloning apereo/cas... done\n",
            "[133/500] Cloning daniulive/SmarterStreaming... done\n",
            "[134/500] Cloning asLody/VirtualApp... done\n",
            "[135/500] Cloning clojure/clojure... done\n",
            "[136/500] Cloning langchain4j/langchain4j... done\n",
            "[137/500] Cloning zfile-dev/zfile... done\n",
            "[138/500] Cloning google/auto... done\n",
            "[139/500] Cloning Activiti/Activiti... done\n",
            "[140/500] Cloning signalapp/Signal-Server... done\n",
            "[141/500] Cloning awsdocs/aws-doc-sdk-examples... done\n",
            "[142/500] Cloning SonarSource/sonarqube... done\n",
            "[143/500] Cloning booklore-app/booklore... done\n",
            "[144/500] Cloning stanfordnlp/CoreNLP... done\n",
            "[145/500] Cloning ZCShou/GoGoGo... done\n",
            "[146/500] Cloning OpenFeign/feign... done\n",
            "[147/500] Cloning iflytek/astron-agent... done\n",
            "[148/500] Cloning spinnaker/spinnaker... done\n",
            "[149/500] Cloning JingMatrix/LSPosed... done\n",
            "[150/500] Cloning apache/cassandra... done\n",
            "[151/500] Cloning flyway/flyway... done\n",
            "[152/500] Cloning AutoMQ/automq... done\n",
            "[153/500] Cloning crossoverJie/cim... done\n",
            "[154/500] Cloning spring-projects/spring-security... done\n",
            "[155/500] Cloning huanghaibin-dev/CalendarView... done\n",
            "[156/500] Cloning android/testing-samples... done\n",
            "[157/500] Cloning apache/jmeter... done\n",
            "[158/500] Cloning apache/seatunnel... done\n",
            "[159/500] Cloning flowable/flowable-engine... done\n",
            "[160/500] Cloning java-native-access/jna... done\n",
            "[161/500] Cloning checkstyle/checkstyle... done\n",
            "[162/500] Cloning HMCL-dev/HMCL... done\n",
            "[163/500] Cloning karatelabs/karate... done\n",
            "[164/500] Cloning apache/shenyu... done\n",
            "[165/500] Cloning cabaletta/baritone... done\n",
            "[166/500] Cloning dropwizard/dropwizard... done\n",
            "[167/500] Cloning testcontainers/testcontainers-java... done\n",
            "[168/500] Cloning airbnb/epoxy... done\n",
            "[169/500] Cloning apache/iceberg... done\n",
            "[170/500] Cloning junit-team/junit4... done\n",
            "[171/500] Cloning apache/beam... done\n",
            "[172/500] Cloning aeron-io/aeron... done\n",
            "[173/500] Cloning wildfirechat/im-server... done\n",
            "[174/500] Cloning alibaba/spring-ai-alibaba... done\n",
            "[175/500] Cloning hs-web/hsweb-framework... done\n",
            "[176/500] Cloning pentaho/pentaho-kettle... done\n",
            "[177/500] Cloning apache/shardingsphere-elasticjob... done\n",
            "[178/500] Cloning apache/tomcat... done\n",
            "[179/500] Cloning junixapp/XPopup... done\n",
            "[180/500] Cloning Graylog2/graylog2-server... done\n",
            "[181/500] Cloning spring-projects/spring-ai... done\n",
            "[182/500] Cloning jeecgboot/jimureport... done\n",
            "[183/500] Cloning dropwizard/metrics... done\n",
            "[184/500] Cloning PowerJob/PowerJob... done\n",
            "[185/500] Cloning AntennaPod/AntennaPod... done\n",
            "[186/500] Cloning mapstruct/mapstruct... done\n",
            "[187/500] Cloning swagger-api/swagger-core... done\n",
            "[188/500] Cloning MinecraftForge/MinecraftForge... done\n",
            "[189/500] Cloning dependency-check/DependencyCheck... done\n",
            "[190/500] Cloning enso-org/enso... done\n",
            "[191/500] Cloning MuntashirAkon/AppManager... done\n",
            "[192/500] Cloning FongMi/TV... done\n",
            "[193/500] Cloning gocd/gocd... done\n",
            "[194/500] Cloning Qihoo360/RePlugin... done\n",
            "[195/500] Cloning didi/KnowStreaming... done\n",
            "[196/500] Cloning Alluxio/alluxio... done\n",
            "[197/500] Cloning wiremock/wiremock... done\n",
            "[198/500] Cloning google/error-prone... done\n",
            "[199/500] Cloning rest-assured/rest-assured... done\n",
            "[200/500] Cloning apache/hertzbeat... done\n",
            "[201/500] Cloning Col-E/Recaf... done\n",
            "[202/500] Cloning junit-team/junit-framework... done\n",
            "[203/500] Cloning traccar/traccar... done\n",
            "[204/500] Cloning getActivity/AndroidProject... done\n",
            "[205/500] Cloning logisim-evolution/logisim-evolution... done\n",
            "[206/500] Cloning Angel-ML/angel... done\n",
            "[207/500] Cloning vespa-engine/vespa... done\n",
            "[208/500] Cloning gzu-liyujiang/AndroidPicker... done\n",
            "[209/500] Cloning raphw/byte-buddy... done\n",
            "[210/500] Cloning iflytek/astron-rpa... done\n",
            "[211/500] Cloning getActivity/XXPermissions... done\n",
            "[212/500] Cloning quartz-scheduler/quartz... done\n",
            "[213/500] Cloning apache/storm... done\n",
            "[214/500] Cloning jOOQ/jOOQ... done\n",
            "[215/500] Cloning 648540858/wvp-GB28181-pro... done\n",
            "[216/500] Cloning CeuiLiSA/Pixiv-Shaft... done\n",
            "[217/500] Cloning apache/zeppelin... done\n",
            "[218/500] Cloning hazelcast/hazelcast... done\n",
            "[219/500] Cloning pig-mesh/pig... done\n",
            "[220/500] Cloning Suwayomi/Suwayomi-Server... done\n",
            "[221/500] Cloning processing/processing... done\n",
            "[222/500] Cloning hibernate/hibernate-orm... done\n",
            "[223/500] Cloning AsyncHttpClient/async-http-client... done\n",
            "[224/500] Cloning micronaut-projects/micronaut-core... done\n",
            "[225/500] Cloning jetlinks/jetlinks-community... done\n",
            "[226/500] Cloning gephi/gephi... done\n",
            "[227/500] Cloning Javen205/IJPay... done\n",
            "[228/500] Cloning joelittlejohn/jsonschema2pojo... done\n",
            "[229/500] Cloning apache/flink-cdc... done\n",
            "[230/500] Cloning haifengl/smile... done\n",
            "[231/500] Cloning apache/iotdb... done\n",
            "[232/500] Cloning graphhopper/graphhopper... done\n",
            "[233/500] Cloning graphql-java/graphql-java... done\n",
            "[234/500] Cloning apache/incubator-kie-drools... done\n",
            "[235/500] Cloning apache/camel... done\n",
            "[236/500] Cloning vavr-io/vavr... done\n",
            "[237/500] Cloning apache/hudi... done\n",
            "[238/500] Cloning javaparser/javaparser... done\n",
            "[239/500] Cloning google/j2objc... done\n",
            "[240/500] Cloning ZhongFuCheng3y/austin... done\n",
            "[241/500] Cloning apache/pinot... done\n",
            "[242/500] Cloning apache/hive... done\n",
            "[243/500] Cloning robolectric/robolectric... done\n",
            "[244/500] Cloning opengoofy/hippo4j... done\n",
            "[245/500] Cloning btraceio/btrace... done\n",
            "[246/500] Cloning apache/nifi... done\n",
            "[247/500] Cloning JSQLParser/JSqlParser... done\n",
            "[248/500] Cloning 00-Evan/shattered-pixel-dungeon... done\n",
            "[249/500] Cloning apache/fesod... done\n",
            "[250/500] Cloning Nepxion/Discovery... done\n",
            "[251/500] Cloning loks666/get_jobs... done\n",
            "[252/500] Cloning Archmage83/tvapk... done\n",
            "[253/500] Cloning dromara/lamp-cloud... done\n",
            "[254/500] Cloning PBH-BTN/PeerBanHelper... done\n",
            "[255/500] Cloning redis/lettuce... done\n",
            "[256/500] Cloning JanusGraph/janusgraph... done\n",
            "[257/500] Cloning strimzi/strimzi-kafka-operator... done\n",
            "[258/500] Cloning 201206030/novel... done\n",
            "[259/500] Cloning TommyLemon/Android-ZBLibrary... done\n",
            "[260/500] Cloning microsoft/typespec... done\n",
            "[261/500] Cloning apache/hbase... done\n",
            "[262/500] Cloning osmandapp/OsmAnd... done\n",
            "[263/500] Cloning hneemann/Digital... done\n",
            "[264/500] Cloning GeyserMC/Geyser... done\n",
            "[265/500] Cloning liquibase/liquibase... done\n",
            "[266/500] Cloning apache/groovy... done\n",
            "[267/500] Cloning spring-projects/spring-data-examples... done\n",
            "[268/500] Cloning jindrapetrik/jpexs-decompiler... done\n",
            "[269/500] Cloning coobird/thumbnailator... done\n",
            "[270/500] Cloning LFDT-web3j/web3j... done\n",
            "[271/500] Cloning tonikelope/megabasterd... done\n",
            "[272/500] Cloning ReVanced/revanced-patches... done\n",
            "[273/500] Cloning SuperMonster003/AutoJs6... done\n",
            "[274/500] Cloning killbill/killbill... done\n",
            "[275/500] Cloning pmd/pmd... done\n",
            "[276/500] Cloning diffplug/spotless... done\n",
            "[277/500] Cloning remkop/picocli... done\n",
            "[278/500] Cloning mybatis/generator... done\n",
            "[279/500] Cloning runelite/runelite... done\n",
            "[280/500] Cloning zfdang/Android-Touch-Helper... done\n",
            "[281/500] Cloning allure-framework/allure2... done\n",
            "[282/500] Cloning TGX-Android/Telegram-X... done\n",
            "[283/500] Cloning LWJGL/lwjgl3... done\n",
            "[284/500] Cloning razerdp/BasePopup... done\n",
            "[285/500] Cloning CellularPrivacy/Android-IMSI-Catcher-Detector... done\n",
            "[286/500] Cloning bitcoinj/bitcoinj... done\n",
            "[287/500] Cloning reactor/reactor-core... done\n",
            "[288/500] Cloning Docile-Alligator/Infinity-For-Reddit... done\n",
            "[289/500] Cloning brianfrankcooper/YCSB... done\n",
            "[290/500] Cloning oshi/oshi... done\n",
            "[291/500] Cloning Kodezi/Chronos... done\n",
            "[292/500] Cloning gsantner/markor... done\n",
            "[293/500] Cloning killme2008/aviatorscript... done\n",
            "[294/500] Cloning tianshiyeben/wgcloud... done\n",
            "[295/500] Cloning apache/calcite... done\n",
            "[296/500] Cloning xuexiangjys/XUI... done\n",
            "[297/500] Cloning line/armeria... done\n",
            "[298/500] Cloning bisq-network/bisq... done\n",
            "[299/500] Cloning apache/ignite... done\n",
            "[300/500] Cloning Sayi/poi-tl... done\n",
            "[301/500] Cloning youlookwhat/CloudReader... done\n",
            "[302/500] Cloning JodaOrg/joda-time... done\n",
            "[303/500] Cloning apache/maven... done\n",
            "[304/500] Cloning spring-cloud/spring-cloud-netflix... done\n",
            "[305/500] Cloning querydsl/querydsl... done\n",
            "[306/500] Cloning rstudio/rstudio... done\n",
            "[307/500] Cloning react-native-webrtc/react-native-webrtc... done\n",
            "[308/500] Cloning orientechnologies/orientdb... done\n",
            "[309/500] Cloning mock-server/mockserver... done\n",
            "[310/500] Cloning angryip/ipscan... done\n",
            "[311/500] Cloning spring-cloud/spring-cloud-gateway... done\n",
            "[312/500] Cloning micrometer-metrics/micrometer... done\n",
            "[313/500] Cloning Threekiii/Awesome-POC... done\n",
            "[314/500] Cloning ageerle/ruoyi-ai... done\n",
            "[315/500] Cloning zhkl0228/unidbg... done\n",
            "[316/500] Cloning deepjavalibrary/djl... done\n",
            "[317/500] Cloning BoCloud/folib... done\n",
            "[318/500] Cloning nisrulz/android-tips-tricks... done\n",
            "[319/500] Cloning oracle/opengrok... done\n",
            "[320/500] Cloning zq2599/blog_demos... done\n",
            "[321/500] Cloning zlt2000/microservices-platform... done\n",
            "[322/500] Cloning dromara/dynamic-tp... done\n",
            "[323/500] Cloning tencentmusic/supersonic... done\n",
            "[324/500] Cloning bytedeco/javacpp... done\n",
            "[325/500] Cloning SPLWare/esProc... done\n",
            "[326/500] Cloning 981011512/--... done\n",
            "[327/500] Cloning Helium314/HeliBoard... done\n",
            "[328/500] Cloning grobidOrg/grobid... done\n",
            "[329/500] Cloning ant-media/Ant-Media-Server... done\n",
            "[330/500] Cloning discord-jda/JDA... done\n",
            "[331/500] Cloning h2database/h2database... done\n",
            "[332/500] Cloning jacoco/jacoco... done\n",
            "[333/500] Cloning 201206030/novel-plus... done\n",
            "[334/500] Cloning nICEnnnnnnnLee/BilibiliDown... done\n",
            "[335/500] Cloning flutter-webrtc/flutter-webrtc... done\n",
            "[336/500] Cloning apache/shiro... done\n",
            "[337/500] Cloning crate/crate... done\n",
            "[338/500] Cloning dragonwell-project/dragonwell8... done\n",
            "[339/500] Cloning jitsi/jitsi... done\n",
            "[340/500] Cloning apache/streampark... done\n",
            "[341/500] Cloning Querz/mcaselector... done\n",
            "[342/500] Cloning ReChronoRain/HyperCeiler... done\n",
            "[343/500] Cloning geoserver/geoserver... done\n",
            "[344/500] Cloning alibaba/fastjson2... done\n",
            "[345/500] Cloning torakiki/pdfsam... done\n",
            "[346/500] Cloning iterate-ch/cyberduck... done\n",
            "[347/500] Cloning microsoft/malmo... done\n",
            "[348/500] Cloning mybatis/spring-boot-starter... done\n",
            "[349/500] Cloning JabRef/jabref... done\n",
            "[350/500] Cloning zendesk/maxwell... done\n",
            "[351/500] Cloning apache/fory... done\n",
            "[352/500] Cloning aws/aws-sdk-java...   [Attempt 1] Timeout exceeded...\n",
            "  [Attempt 2] Failed. Git says: fatal: destination path 'dataset\\java_repos\\aws_aws-sdk-java' already exists and is not an empty directory.\n",
            "  [Attempt 3] Failed. Git says: fatal: destination path 'dataset\\java_repos\\aws_aws-sdk-java' already exists and is not an empty directory.\n",
            "failed\n",
            "[353/500] Cloning M66B/FairEmail... done\n",
            "[354/500] Cloning LibrePDF/OpenPDF... done\n",
            "[355/500] Cloning jMonkeyEngine/jmonkeyengine... done\n",
            "[356/500] Cloning googlesamples/mlkit... done\n",
            "[357/500] Cloning pqpo/SmartCropper... done\n",
            "[358/500] Cloning DTStack/chunjun... done\n",
            "[359/500] Cloning lilishop/lilishop... done\n",
            "[360/500] Cloning lukas-krecan/ShedLock... done\n",
            "[361/500] Cloning bitcoin-wallet/bitcoin-wallet... done\n",
            "[362/500] Cloning jishenghua/jshERP... done\n",
            "[363/500] Cloning JFormDesigner/FlatLaf... done\n",
            "[364/500] Cloning jetty/jetty.project... done\n",
            "[365/500] Cloning knowm/XChange... done\n",
            "[366/500] Cloning gh0stkey/HaE... done\n",
            "[367/500] Cloning tronprotocol/java-tron... done\n",
            "[368/500] Cloning camunda/camunda... done\n",
            "[369/500] Cloning Creators-of-Create/Create... done\n",
            "[370/500] Cloning mercyblitz/tech-weekly... done\n",
            "[371/500] Cloning freeplane/freeplane... done\n",
            "[372/500] Cloning google/bundletool... done\n",
            "[373/500] Cloning kubernetes-client/java... done\n",
            "[374/500] Cloning Dimezis/BlurView... done\n",
            "[375/500] Cloning vipulasri/Timeline-View... done\n",
            "[376/500] Cloning shopizer-ecommerce/shopizer... done\n",
            "[377/500] Cloning react-native-share/react-native-share... done\n",
            "[378/500] Cloning MovingBlocks/Terasology... done\n",
            "[379/500] Cloning spotbugs/spotbugs... done\n",
            "[380/500] Cloning yacy/yacy_search_server... done\n",
            "[381/500] Cloning RoaringBitmap/RoaringBitmap... done\n",
            "[382/500] Cloning uncle-novel/uncle-novel... done\n",
            "[383/500] Cloning blossom-editor/blossom... done\n",
            "[384/500] Cloning zhpanvip/BannerViewPager... done\n",
            "[385/500] Cloning tchiotludo/akhq... done\n",
            "[386/500] Cloning apache/kylin... done\n",
            "[387/500] Cloning rbmonster/learning-note... done\n",
            "[388/500] Cloning undertow-io/undertow... done\n",
            "[389/500] Cloning helidon-io/helidon... done\n",
            "[390/500] Cloning jtablesaw/tablesaw... done\n",
            "[391/500] Cloning Atmosphere/atmosphere... done\n",
            "[392/500] Cloning emanuele-f/PCAPdroid... done\n",
            "[393/500] Cloning DataLinkDC/dinky... done\n",
            "[394/500] Cloning OpenHFT/Chronicle-Queue... done\n",
            "[395/500] Cloning FasterXML/jackson-databind... done\n",
            "[396/500] Cloning networknt/light-4j... done\n",
            "[397/500] Cloning springdoc/springdoc-openapi... done\n",
            "[398/500] Cloning portfolio-performance/portfolio... done\n",
            "[399/500] Cloning IrisShaders/Iris... done\n",
            "[400/500] Cloning 1024-lab/smart-admin... done\n",
            "[401/500] Cloning dromara/liteflow... done\n",
            "[402/500] Cloning anggrayudi/android-hidden-api... done\n",
            "[403/500] Cloning spring-io/initializr... done\n",
            "[404/500] Cloning spockframework/spock... done\n",
            "[405/500] Cloning fabric8io/kubernetes-client... done\n",
            "[406/500] Cloning DependencyTrack/dependency-track... done\n",
            "[407/500] Cloning FCL-Team/FoldCraftLauncher... done\n",
            "[408/500] Cloning dtinit/data-transfer-project... done\n",
            "[409/500] Cloning TNG/ArchUnit... done\n",
            "[410/500] Cloning googlemaps/android-maps-utils... done\n",
            "[411/500] Cloning apache/logging-log4j2... done\n",
            "[412/500] Cloning vert-x3/vertx-examples... done\n",
            "[413/500] Cloning apache/tika... done\n",
            "[414/500] Cloning immutables/immutables... done\n",
            "[415/500] Cloning AxonFramework/AxonFramework... done\n",
            "[416/500] Cloning spring-cloud/spring-cloud-kubernetes... done\n",
            "[417/500] Cloning eclipse-openj9/openj9... done\n",
            "[418/500] Cloning NotHarshhaa/DevOps-Projects... done\n",
            "[419/500] Cloning apache/incubator-kie-optaplanner... done\n",
            "[420/500] Cloning getActivity/Toaster... done\n",
            "[421/500] Cloning M66B/NetGuard... done\n",
            "[422/500] Cloning Athou/commafeed... done\n",
            "[423/500] Cloning GitLqr/LQRWeChat... done\n",
            "[424/500] Cloning apache/linkis... done\n",
            "[425/500] Cloning EngineHub/WorldEdit... done\n",
            "[426/500] Cloning aeron-io/simple-binary-encoding... done\n",
            "[427/500] Cloning niumoo/bing-wallpaper... done\n",
            "[428/500] Cloning Neamar/KISS... done\n",
            "[429/500] Cloning Nekogram/Nekogram... done\n",
            "[430/500] Cloning apache/lucene... done\n",
            "[431/500] Cloning wix/react-native-notifications... done\n",
            "[432/500] Cloning dunwu/javacore... done\n",
            "[433/500] Cloning jenly1314/ZXingLite... done\n",
            "[434/500] Cloning unitycatalog/unitycatalog... done\n",
            "[435/500] Cloning openrewrite/rewrite... done\n",
            "[436/500] Cloning prometheus/jmx_exporter... done\n",
            "[437/500] Cloning ukanth/afwall... done\n",
            "[438/500] Cloning WeBankFinTech/DataSphereStudio... done\n",
            "[439/500] Cloning MeteorDevelopment/meteor-client... done\n",
            "[440/500] Cloning spring-projects/spring-data-jpa... done\n",
            "[441/500] Cloning AnySoftKeyboard/AnySoftKeyboard... done\n",
            "[442/500] Cloning apache/avro... done\n",
            "[443/500] Cloning lakesoul-io/LakeSoul... done\n",
            "[444/500] Cloning proninyaroslav/libretorrent... done\n",
            "[445/500] Cloning florent37/ShapeOfView... done\n",
            "[446/500] Cloning qos-ch/logback... done\n",
            "[447/500] Cloning xerial/sqlite-jdbc... done\n",
            "[448/500] Cloning internetarchive/heritrix3... done\n",
            "[449/500] Cloning oracle/visualvm... done\n",
            "[450/500] Cloning apache/paimon... done\n",
            "[451/500] Cloning smartloli/EFAK... done\n",
            "[452/500] Cloning apache/curator... done\n",
            "[453/500] Cloning wildfly/wildfly... done\n",
            "[454/500] Cloning RaiMan/SikuliX1... done\n",
            "[455/500] Cloning Wisser/Jailer... done\n",
            "[456/500] Cloning apache/nutch... done\n",
            "[457/500] Cloning egzosn/pay-java-parent... done\n",
            "[458/500] Cloning CrawlScript/WebCollector... done\n",
            "[459/500] Cloning infinilabs/analysis-pinyin... done\n",
            "[460/500] Cloning soot-oss/soot... done\n",
            "[461/500] Cloning Minestom/Minestom... done\n",
            "[462/500] Cloning javamelody/javamelody... done\n",
            "[463/500] Cloning apache/parquet-java... done\n",
            "[464/500] Cloning apache/pdfbox... done\n",
            "[465/500] Cloning apache/netbeans... done\n",
            "[466/500] Cloning sleuthkit/autopsy... done\n",
            "[467/500] Cloning FabricMC/fabric-api... done\n",
            "[468/500] Cloning igniterealtime/Openfire... done\n",
            "[469/500] Cloning konsoletyper/teavm... done\n",
            "[470/500] Cloning linkedin/cruise-control... done\n",
            "[471/500] Cloning itwanger/paicoding... done\n",
            "[472/500] Cloning WindySha/Xpatch... done\n",
            "[473/500] Cloning synthetichealth/synthea... done\n",
            "[474/500] Cloning alldatacenter/alldata... done\n",
            "[475/500] Cloning apache/incubator-hugegraph... done\n",
            "[476/500] Cloning power721/alist-tvbox... done\n",
            "[477/500] Cloning spring-projects/spring-data-elasticsearch... done\n",
            "[478/500] Cloning classgraph/classgraph... done\n",
            "[479/500] Cloning presmihaylov/booknotes... done\n",
            "[480/500] Cloning wkeyuan/DWSurvey... done\n",
            "[481/500] Cloning algorithmzuo/algorithm-journey... done\n",
            "[482/500] Cloning thymeleaf/thymeleaf... done\n",
            "[483/500] Cloning apache/commons-lang... done\n",
            "[484/500] Cloning OpenHFT/Chronicle-Map... done\n",
            "[485/500] Cloning Tornaco/Thanox... done\n",
            "[486/500] Cloning polymorphicshade/Tubular... done\n",
            "[487/500] Cloning spring-projects/spring-batch... done\n",
            "[488/500] Cloning mybatis/spring... done\n",
            "[489/500] Cloning google/bindiff... done\n",
            "[490/500] Cloning 88250/symphony... done\n",
            "[491/500] Cloning starcwang/easy_javadoc... done\n",
            "[492/500] Cloning jenkinsci/blueocean-plugin... done\n",
            "[493/500] Cloning jobrunr/jobrunr... done\n",
            "[494/500] Cloning Eanya-Tonic/CCTV_Viewer... done\n",
            "[495/500] Cloning apache/gravitino... done\n",
            "[496/500] Cloning WhatsApp/stickers... done\n",
            "[497/500] Cloning bytedeco/javacpp-presets... done\n",
            "[498/500] Cloning assertj/assertj... done\n",
            "[499/500] Cloning sqlcipher/android-database-sqlcipher... done\n",
            "[500/500] Cloning apache/cloudstack... done\n",
            "\n",
            "Summary:\n",
            "  Successfully cloned: 499\n",
            "  Failed: 1\n"
          ]
        }
      ],
      "source": [
        "# Standard Windows installation path for Git\n",
        "GIT_EXE = r\"C:\\Program Files\\Git\\cmd\\git.exe\"\n",
        "\n",
        "def remove_readonly(func, path, excinfo):\n",
        "    \"\"\"Error handler for shutil.onexc to fix Access Denied errors.\"\"\"\n",
        "    try:\n",
        "        os.chmod(path, stat.S_IWRITE)\n",
        "        func(path)\n",
        "    except Exception:\n",
        "        pass # Ignore if we still can't delete it\n",
        "\n",
        "def clone_repo(clone_url, dest_dir, max_retries=3):\n",
        "    \"\"\"\n",
        "    Shallow clone a repository with long-path support and error visibility.\n",
        "    \"\"\"\n",
        "    for attempt in range(1, max_retries + 1):\n",
        "        try:\n",
        "            # Clean up directory from a previous failed attempt\n",
        "            if os.path.exists(dest_dir):\n",
        "                shutil.onexc(dest_dir, onerror=remove_readonly)\n",
        "\n",
        "            git_cmd = GIT_EXE if os.path.exists(GIT_EXE) else \"git\"\n",
        "            \n",
        "            # Used to bypass 260-char limits\n",
        "            cmd = f'\"{git_cmd}\" clone -c core.longpaths=true --depth 1 --quiet {clone_url} \"{dest_dir}\"'\n",
        "            \n",
        "            result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=240)\n",
        "\n",
        "            if result.returncode == 0:\n",
        "                return True\n",
        "            else:\n",
        "                print(f\"  [Attempt {attempt}] Failed. Git says: {result.stderr.strip()}\")\n",
        "                \n",
        "        except subprocess.TimeoutExpired:\n",
        "            print(f\"  [Attempt {attempt}] Timeout exceeded...\")\n",
        "        except Exception as e:\n",
        "            print(f\"  [Attempt {attempt}] Python Error: {e}\")\n",
        "            \n",
        "        # Brief pause before retrying to give API limit breathing room\n",
        "        if attempt < max_retries:\n",
        "            time.sleep(3)\n",
        "\n",
        "    return False\n",
        "\n",
        "# Clone repositories\n",
        "cloned_repos = []\n",
        "failed_repos = []\n",
        "\n",
        "\n",
        "print(f\"Cloning repositories ...\\n\")\n",
        "\n",
        "for idx, row in df_repos.iterrows():\n",
        "    repo_name = row[\"full_name\"]\n",
        "    clone_url = row[\"clone_url\"]\n",
        "\n",
        "    safe_name = repo_name.replace(\"/\", \"_\")\n",
        "    dest_dir = os.path.join(CLONE_DIR, safe_name)\n",
        "\n",
        "    print(f\"[{idx+1}/{len(df_repos)}] Cloning {repo_name}...\", end=\" \")\n",
        "\n",
        "    success = clone_repo(clone_url, dest_dir)\n",
        "\n",
        "    if success:\n",
        "        cloned_repos.append({\n",
        "            \"repo_name\": repo_name,\n",
        "            \"local_path\": dest_dir,\n",
        "            \"stars\": row[\"stars\"]\n",
        "        })\n",
        "        print(\"done\")\n",
        "    else:\n",
        "        failed_repos.append(repo_name)\n",
        "        print(\"failed\")\n",
        "\n",
        "print(f\"\\nSummary:\")\n",
        "print(f\"  Successfully cloned: {len(cloned_repos)}\")\n",
        "print(f\"  Failed: {len(failed_repos)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxahpQp-acyG"
      },
      "source": [
        "---\n",
        "## Find and Select Java Files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeGllaFGacyG",
        "outputId": "7d177ac9-827c-47fb-c48b-10807494b818"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finding Java files (selecting up to 10 per repo)...\n",
            "\n",
            "  iluwatar/java-design-patterns: 10/1262 files selected\n",
            "  macrozheng/mall: 10/508 files selected\n",
            "  spring-projects/spring-boot: 10/3721 files selected\n",
            "  doocs/advanced-java: 1/1 files selected\n",
            "  elastic/elasticsearch: 10/14081 files selected\n",
            "  NationalSecurityAgency/ghidra: 10/13281 files selected\n",
            "  spring-projects/spring-framework: 10/4929 files selected\n",
            "  google/guava: 10/1270 files selected\n",
            "  dbeaver/dbeaver: 10/5986 files selected\n",
            "  ReactiveX/RxJava: 10/914 files selected\n",
            "  skylot/jadx: 10/1209 files selected\n",
            "  jeecgboot/JeecgBoot: 10/786 files selected\n",
            "  apache/dubbo: 10/2390 files selected\n",
            "  PhilJay/MPAndroidChart: 10/158 files selected\n",
            "  halo-dev/halo: 10/896 files selected\n",
            "  eugenp/tutorials: 10/14951 files selected\n",
            "  TeamNewPipe/NewPipe: 10/290 files selected\n",
            "  alibaba/arthas: 10/756 files selected\n",
            "  airbnb/lottie-android: 10/215 files selected\n",
            "  YunaiV/ruoyi-vue-pro: 10/3548 files selected\n",
            "  bumptech/glide: 10/396 files selected\n",
            "  netty/netty: 10/2167 files selected\n",
            "  SeleniumHQ/selenium: 10/912 files selected\n",
            "  zxing/zxing: 10/359 files selected\n",
            "  keycloak/keycloak: 10/4904 files selected\n",
            "  alibaba/nacos: 10/2123 files selected\n",
            "  binarywang/WxJava: 10/3456 files selected\n",
            "  apache/kafka: 10/3543 files selected\n",
            "  conductor-oss/conductor: 10/603 files selected\n",
            "  chinabugotech/hutool: 10/1554 files selected\n",
            "  xuxueli/xxl-job: 10/125 files selected\n",
            "  apolloconfig/apollo: 10/501 files selected\n",
            "  alibaba/canal: 10/560 files selected\n",
            "  alibaba/spring-cloud-alibaba: 10/227 files selected\n",
            "  DrKLO/Telegram: 10/2905 files selected\n",
            "  yuliskov/SmartTube: 10/1384 files selected\n",
            "  alibaba/druid: 10/1696 files selected\n",
            "  Anuken/Mindustry: 10/806 files selected\n",
            "  qiurunze123/miaosha: 10/275 files selected\n",
            "  kestra-io/kestra: 10/1258 files selected\n",
            "  apache/incubator-seata: 10/1635 files selected\n",
            "  OpenAPITools/openapi-generator: 10/407 files selected\n",
            "  apache/flink: 10/8984 files selected\n",
            "  CodePhiliaX/Chat2DB: 10/953 files selected\n",
            "  bazelbuild/bazel: 10/3858 files selected\n",
            "  jenkinsci/jenkins: 10/1250 files selected\n",
            "  libgdx/libgdx: 10/2005 files selected\n",
            "  apache/skywalking: 10/1939 files selected\n",
            "  google/gson: 10/101 files selected\n",
            "  redisson/redisson: 10/2584 files selected\n",
            "  iBotPeaches/Apktool: 10/88 files selected\n",
            "  dataease/dataease: 10/1006 files selected\n",
            "  doocs/source-code-hunter: 1/1 files selected\n",
            "  LSPosed/LSPosed: 10/230 files selected\n",
            "  openjdk/jdk: 10/14180 files selected\n",
            "  apache/rocketmq: 10/1486 files selected\n",
            "  google/ExoPlayer: 10/1048 files selected\n",
            "  oracle/graal: 10/11792 files selected\n",
            "  CarGuo/GSYVideoPlayer: 10/143 files selected\n",
            "  thingsboard/thingsboard: 10/3708 files selected\n",
            "  apache/shardingsphere: 10/4502 files selected\n",
            "  didi/DoKit: 10/688 files selected\n",
            "  mybatis/mybatis-3: 10/394 files selected\n",
            "  linlinjava/litemall: 10/304 files selected\n",
            "  JetBrains/intellij-community: 10/35647 files selected\n",
            "  dianping/cat: 10/1175 files selected\n",
            "  antlr/antlr4: 10/410 files selected\n",
            "  lionsoul2014/ip2region: 10/28 files selected\n",
            "  YunaiV/yudao-cloud: 10/3644 files selected\n",
            "  williamfiset/Algorithms: 10/220 files selected\n",
            "  Tencent/APIJSON: 10/68 files selected\n",
            "  LMAX-Exchange/disruptor: 10/88 files selected\n",
            "  ben-manes/caffeine: 10/363 files selected\n",
            "  infinilabs/analysis-ik: 10/31 files selected\n",
            "  openzipkin/zipkin: 10/225 files selected\n",
            "  baomidou/mybatis-plus: 10/479 files selected\n",
            "  material-components/material-components-android: 10/717 files selected\n",
            "  facebook/fresco: 10/317 files selected\n",
            "  alibaba/DataX: 10/824 files selected\n",
            "  Grasscutters/Grasscutter: 10/1536 files selected\n",
            "  questdb/questdb: 10/3336 files selected\n",
            "  prestodb/presto: 10/6444 files selected\n",
            "  neo4j/neo4j: 10/4873 files selected\n",
            "  apache/hadoop: 10/7918 files selected\n",
            "  quarkusio/quarkus: 10/7561 files selected\n",
            "  mockito/mockito: 10/492 files selected\n",
            "  Konloch/bytecode-viewer: 10/305 files selected\n",
            "  apache/pulsar: 10/2649 files selected\n",
            "  zhisheng17/flink-learning: 10/312 files selected\n",
            "  apache/doris: 10/5003 files selected\n",
            "  supertokens/supertokens-core: 10/396 files selected\n",
            "  elastic/logstash: 10/418 files selected\n",
            "  zaproxy/zaproxy: 10/1137 files selected\n",
            "  theonedev/onedev: 10/3573 files selected\n",
            "  eclipse-vertx/vert.x: 10/693 files selected\n",
            "  cryptomator/cryptomator: 10/379 files selected\n",
            "  arduino/Arduino: 10/235 files selected\n",
            "  GoogleContainerTools/jib: 10/321 files selected\n",
            "  deeplearning4j/deeplearning4j: 10/3495 files selected\n",
            "  apache/dolphinscheduler: 10/2027 files selected\n",
            "  languagetool-org/languagetool: 10/1294 files selected\n",
            "  kekingcn/kkFileView: 10/75 files selected\n",
            "  apache/druid: 10/5784 files selected\n",
            "  janishar/mit-deep-learning-book-pdf: 1/1 files selected\n",
            "  pinpoint-apm/pinpoint: 10/5321 files selected\n",
            "  xpipe-io/xpipe: 10/969 files selected\n",
            "  projectlombok/lombok: 10/439 files selected\n",
            "  metersphere/metersphere: 10/1722 files selected\n",
            "  macrozheng/mall-swarm: 10/508 files selected\n",
            "  codecentric/spring-boot-admin: 10/204 files selected\n",
            "  apache/zookeeper: 10/519 files selected\n",
            "  google/guice: 10/391 files selected\n",
            "  Netflix/eureka: 10/284 files selected\n",
            "  plantuml/plantuml: 10/3242 files selected\n",
            "  trinodb/trino: 10/6848 files selected\n",
            "  debezium/debezium: 10/1478 files selected\n",
            "  opensearch-project/OpenSearch: 10/6682 files selected\n",
            "  microg/GmsCore: 10/1469 files selected\n",
            "  redis/jedis: 10/475 files selected\n",
            "  LawnchairLauncher/lawnchair: 10/1367 files selected\n",
            "  Yalantis/uCrop: 10/31 files selected\n",
            "  grpc/grpc-java: 10/845 files selected\n",
            "  beemdevelopment/Aegis: 10/233 files selected\n",
            "  PaperMC/Paper: 10/3060 files selected\n",
            "  OpenRefine/OpenRefine: 10/694 files selected\n",
            "  code4craft/webmagic: 10/140 files selected\n",
            "  datahub-project/datahub: 10/2267 files selected\n",
            "  newbee-ltd/newbee-mall: 10/88 files selected\n",
            "  realm/realm-java: 10/274 files selected\n",
            "  StarRocks/starrocks: 10/3973 files selected\n",
            "  jd-opensource/joyagent-jdgenie: 10/188 files selected\n",
            "  apereo/cas: 10/4705 files selected\n",
            "  daniulive/SmarterStreaming: 10/22 files selected\n",
            "  asLody/VirtualApp: 10/553 files selected\n",
            "  clojure/clojure: 10/173 files selected\n",
            "  langchain4j/langchain4j: 10/1521 files selected\n",
            "  zfile-dev/zfile: 10/415 files selected\n",
            "  google/auto: 10/132 files selected\n",
            "  Activiti/Activiti: 10/2055 files selected\n",
            "  signalapp/Signal-Server: 10/795 files selected\n",
            "  awsdocs/aws-doc-sdk-examples: No Java files found\n",
            "  SonarSource/sonarqube: 10/5497 files selected\n",
            "  booklore-app/booklore: 10/804 files selected\n",
            "  stanfordnlp/CoreNLP: 10/1751 files selected\n",
            "  ZCShou/GoGoGo: 10/16 files selected\n",
            "  OpenFeign/feign: 10/252 files selected\n",
            "  iflytek/astron-agent: 10/1125 files selected\n",
            "  spinnaker/spinnaker: 10/5536 files selected\n",
            "  JingMatrix/LSPosed: 10/227 files selected\n",
            "  apache/cassandra: 10/3293 files selected\n",
            "  flyway/flyway: 10/728 files selected\n",
            "  AutoMQ/automq: 10/3916 files selected\n",
            "  crossoverJie/cim: 10/134 files selected\n",
            "  spring-projects/spring-security: 10/2450 files selected\n",
            "  huanghaibin-dev/CalendarView: 10/86 files selected\n",
            "  android/testing-samples: No Java files found\n",
            "  apache/jmeter: 10/966 files selected\n",
            "  apache/seatunnel: 10/3319 files selected\n",
            "  flowable/flowable-engine: 10/5404 files selected\n",
            "  java-native-access/jna: 10/386 files selected\n",
            "  checkstyle/checkstyle: 10/1164 files selected\n",
            "  HMCL-dev/HMCL: 10/815 files selected\n",
            "  karatelabs/karate: 10/266 files selected\n",
            "  apache/shenyu: 10/1887 files selected\n",
            "  cabaletta/baritone: 10/353 files selected\n",
            "  dropwizard/dropwizard: 10/393 files selected\n",
            "  testcontainers/testcontainers-java: No Java files found\n",
            "  airbnb/epoxy: 10/85 files selected\n",
            "  apache/iceberg: 10/3192 files selected\n",
            "  junit-team/junit4: 10/219 files selected\n",
            "  apache/beam: 10/3894 files selected\n",
            "  aeron-io/aeron: 10/503 files selected\n",
            "  wildfirechat/im-server: 10/601 files selected\n",
            "  alibaba/spring-ai-alibaba: 10/1188 files selected\n",
            "  hs-web/hsweb-framework: 10/593 files selected\n",
            "  pentaho/pentaho-kettle: 10/3638 files selected\n",
            "  apache/shardingsphere-elasticjob: 10/275 files selected\n",
            "  apache/tomcat: 10/1812 files selected\n",
            "  junixapp/XPopup: 10/78 files selected\n",
            "  Graylog2/graylog2-server: 10/4362 files selected\n",
            "  spring-projects/spring-ai: 10/1152 files selected\n",
            "  jeecgboot/jimureport: No Java files found\n",
            "  dropwizard/metrics: 10/195 files selected\n",
            "  PowerJob/PowerJob: 10/570 files selected\n",
            "  AntennaPod/AntennaPod: 10/524 files selected\n",
            "  mapstruct/mapstruct: 10/403 files selected\n",
            "  swagger-api/swagger-core: 10/256 files selected\n",
            "  MinecraftForge/MinecraftForge: 10/787 files selected\n",
            "  dependency-check/DependencyCheck: 10/331 files selected\n",
            "  enso-org/enso: 10/1530 files selected\n",
            "  MuntashirAkon/AppManager: 10/903 files selected\n",
            "  FongMi/TV: 10/405 files selected\n",
            "  gocd/gocd: 10/2414 files selected\n",
            "  Qihoo360/RePlugin: 10/232 files selected\n",
            "  didi/KnowStreaming: 10/895 files selected\n",
            "  Alluxio/alluxio: 10/1948 files selected\n",
            "  wiremock/wiremock: 10/819 files selected\n",
            "  google/error-prone: 10/1044 files selected\n",
            "  rest-assured/rest-assured: 10/225 files selected\n",
            "  apache/hertzbeat: 10/940 files selected\n",
            "  Col-E/Recaf: 10/1140 files selected\n",
            "  junit-team/junit-framework: 10/867 files selected\n",
            "  traccar/traccar: 10/1020 files selected\n",
            "  getActivity/AndroidProject: 10/49 files selected\n",
            "  logisim-evolution/logisim-evolution: 10/1147 files selected\n",
            "  Angel-ML/angel: 10/1552 files selected\n",
            "  vespa-engine/vespa: 10/5421 files selected\n",
            "  gzu-liyujiang/AndroidPicker: 10/169 files selected\n",
            "  raphw/byte-buddy: 10/423 files selected\n",
            "  iflytek/astron-rpa: 10/905 files selected\n",
            "  getActivity/XXPermissions: 10/98 files selected\n",
            "  quartz-scheduler/quartz: 10/236 files selected\n",
            "  apache/storm: 10/1372 files selected\n",
            "  jOOQ/jOOQ: 10/2666 files selected\n",
            "  648540858/wvp-GB28181-pro: 10/746 files selected\n",
            "  CeuiLiSA/Pixiv-Shaft: 10/423 files selected\n",
            "  apache/zeppelin: 10/563 files selected\n",
            "  hazelcast/hazelcast: 10/6845 files selected\n",
            "  pig-mesh/pig: 10/384 files selected\n",
            "  Suwayomi/Suwayomi-Server: 10/280 files selected\n",
            "  processing/processing: 10/257 files selected\n",
            "  hibernate/hibernate-orm: 10/6731 files selected\n",
            "  AsyncHttpClient/async-http-client: 10/183 files selected\n",
            "  micronaut-projects/micronaut-core: 10/2429 files selected\n",
            "  jetlinks/jetlinks-community: 10/1378 files selected\n",
            "  gephi/gephi: 10/1330 files selected\n",
            "  Javen205/IJPay: 10/195 files selected\n",
            "  joelittlejohn/jsonschema2pojo: 10/94 files selected\n",
            "  apache/flink-cdc: 10/1015 files selected\n",
            "  haifengl/smile: 10/1001 files selected\n",
            "  apache/iotdb: 10/5517 files selected\n",
            "  graphhopper/graphhopper: 10/663 files selected\n",
            "  graphql-java/graphql-java: 10/665 files selected\n",
            "  apache/incubator-kie-drools: 10/5989 files selected\n",
            "  apache/camel: 10/9063 files selected\n",
            "  vavr-io/vavr: 10/97 files selected\n",
            "  apache/hudi: 10/2560 files selected\n",
            "  javaparser/javaparser: 10/789 files selected\n",
            "  google/j2objc: 10/3236 files selected\n",
            "  ZhongFuCheng3y/austin: 10/288 files selected\n",
            "  apache/pinot: 10/3682 files selected\n",
            "  apache/hive: 10/6336 files selected\n",
            "  robolectric/robolectric: 10/1147 files selected\n",
            "  opengoofy/hippo4j: 10/809 files selected\n",
            "  btraceio/btrace: 10/337 files selected\n",
            "  apache/nifi: 10/6315 files selected\n",
            "  JSQLParser/JSqlParser: 10/482 files selected\n",
            "  00-Evan/shattered-pixel-dungeon: 10/1282 files selected\n",
            "  apache/fesod: 10/335 files selected\n",
            "  Nepxion/Discovery: 10/622 files selected\n",
            "  loks666/get_jobs: 10/84 files selected\n",
            "  Archmage83/tvapk: No Java files found\n",
            "  dromara/lamp-cloud: 10/760 files selected\n",
            "  PBH-BTN/PeerBanHelper: 10/619 files selected\n",
            "  redis/lettuce: 10/998 files selected\n",
            "  JanusGraph/janusgraph: 10/951 files selected\n",
            "  strimzi/strimzi-kafka-operator: 10/652 files selected\n",
            "  201206030/novel: 10/187 files selected\n",
            "  TommyLemon/Android-ZBLibrary: 10/107 files selected\n",
            "  microsoft/typespec: 10/437 files selected\n",
            "  apache/hbase: 10/2647 files selected\n",
            "  osmandapp/OsmAnd: 10/3148 files selected\n",
            "  hneemann/Digital: 10/800 files selected\n",
            "  GeyserMC/Geyser: 10/1217 files selected\n",
            "  liquibase/liquibase: 10/1148 files selected\n",
            "  apache/groovy: 10/1463 files selected\n",
            "  spring-projects/spring-data-examples: No Java files found\n",
            "  jindrapetrik/jpexs-decompiler: 10/2604 files selected\n",
            "  coobird/thumbnailator: 10/79 files selected\n",
            "  LFDT-web3j/web3j: 10/414 files selected\n",
            "  tonikelope/megabasterd: 10/61 files selected\n",
            "  ReVanced/revanced-patches: 10/373 files selected\n",
            "  SuperMonster003/AutoJs6: 10/1063 files selected\n",
            "  killbill/killbill: 10/1080 files selected\n",
            "  pmd/pmd: 10/1761 files selected\n",
            "  diffplug/spotless: 10/378 files selected\n",
            "  remkop/picocli: 10/59 files selected\n",
            "  mybatis/generator: 10/413 files selected\n",
            "  runelite/runelite: 10/1693 files selected\n",
            "  zfdang/Android-Touch-Helper: 10/15 files selected\n",
            "  allure-framework/allure2: 10/209 files selected\n",
            "  TGX-Android/Telegram-X: 10/1086 files selected\n",
            "  LWJGL/lwjgl3: 10/181 files selected\n",
            "  razerdp/BasePopup: 10/37 files selected\n",
            "  CellularPrivacy/Android-IMSI-Catcher-Detector: 10/103 files selected\n",
            "  bitcoinj/bitcoinj: 10/296 files selected\n",
            "  reactor/reactor-core: 10/519 files selected\n",
            "  Docile-Alligator/Infinity-For-Reddit: 10/618 files selected\n",
            "  brianfrankcooper/YCSB: 10/185 files selected\n",
            "  oshi/oshi: 10/451 files selected\n",
            "  Kodezi/Chronos: 10/47305 files selected\n",
            "  gsantner/markor: 10/136 files selected\n",
            "  killme2008/aviatorscript: 10/283 files selected\n",
            "  tianshiyeben/wgcloud: 10/117 files selected\n",
            "  apache/calcite: 10/2097 files selected\n",
            "  xuexiangjys/XUI: 10/452 files selected\n",
            "  line/armeria: 10/2512 files selected\n",
            "  bisq-network/bisq: 10/2105 files selected\n",
            "  apache/ignite: 10/5687 files selected\n",
            "  Sayi/poi-tl: 10/243 files selected\n",
            "  youlookwhat/CloudReader: 10/31 files selected\n",
            "  JodaOrg/joda-time: 10/166 files selected\n",
            "  apache/maven: 10/1577 files selected\n",
            "  spring-cloud/spring-cloud-netflix: 10/69 files selected\n",
            "  querydsl/querydsl: 10/854 files selected\n",
            "  rstudio/rstudio: 10/2655 files selected\n",
            "  react-native-webrtc/react-native-webrtc: 10/33 files selected\n",
            "  orientechnologies/orientdb: 10/2741 files selected\n",
            "  mock-server/mockserver: 10/494 files selected\n",
            "  angryip/ipscan: 10/162 files selected\n",
            "  spring-cloud/spring-cloud-gateway: 10/377 files selected\n",
            "  micrometer-metrics/micrometer: 10/729 files selected\n",
            "  Threekiii/Awesome-POC: No Java files found\n",
            "  ageerle/ruoyi-ai: 10/987 files selected\n",
            "  zhkl0228/unidbg: 10/738 files selected\n",
            "  deepjavalibrary/djl: 10/1029 files selected\n",
            "  BoCloud/folib: 10/2413 files selected\n",
            "  nisrulz/android-tips-tricks: 1/1 files selected\n",
            "  oracle/opengrok: 10/592 files selected\n",
            "  zq2599/blog_demos: No Java files found\n",
            "  zlt2000/microservices-platform: 10/278 files selected\n",
            "  dromara/dynamic-tp: 10/330 files selected\n",
            "  tencentmusic/supersonic: 10/1005 files selected\n",
            "  bytedeco/javacpp: 10/185 files selected\n",
            "  SPLWare/esProc: 10/1917 files selected\n",
            "  981011512/--: 10/1973 files selected\n",
            "  Helium314/HeliBoard: 10/156 files selected\n",
            "  grobidOrg/grobid: 10/317 files selected\n",
            "  ant-media/Ant-Media-Server: 10/775 files selected\n",
            "  discord-jda/JDA: 10/1171 files selected\n",
            "  h2database/h2database: 10/913 files selected\n",
            "  jacoco/jacoco: 10/265 files selected\n",
            "  201206030/novel-plus: 10/391 files selected\n",
            "  nICEnnnnnnnLee/BilibiliDown: 10/171 files selected\n",
            "  flutter-webrtc/flutter-webrtc: 10/52 files selected\n",
            "  apache/shiro: 10/585 files selected\n",
            "  crate/crate: 10/3564 files selected\n",
            "  dragonwell-project/dragonwell8: 10/17470 files selected\n",
            "  jitsi/jitsi: 10/1739 files selected\n",
            "  apache/streampark: 10/406 files selected\n",
            "  Querz/mcaselector: 10/304 files selected\n",
            "  ReChronoRain/HyperCeiler: 10/687 files selected\n",
            "  geoserver/geoserver: 10/5840 files selected\n",
            "  alibaba/fastjson2: 10/836 files selected\n",
            "  torakiki/pdfsam: 10/377 files selected\n",
            "  iterate-ch/cyberduck: 10/3299 files selected\n",
            "  microsoft/malmo: 10/136 files selected\n",
            "  mybatis/spring-boot-starter: 10/10 files selected\n",
            "  JabRef/jabref: 10/1992 files selected\n",
            "  zendesk/maxwell: 10/195 files selected\n",
            "  apache/fory: 10/431 files selected\n",
            "  M66B/FairEmail: 10/828 files selected\n",
            "  LibrePDF/OpenPDF: 10/983 files selected\n",
            "  jMonkeyEngine/jmonkeyengine: 10/1324 files selected\n",
            "  googlesamples/mlkit: No Java files found\n",
            "  pqpo/SmartCropper: 7/7 files selected\n",
            "  DTStack/chunjun: 10/1462 files selected\n",
            "  lilishop/lilishop: 10/1527 files selected\n",
            "  lukas-krecan/ShedLock: 10/192 files selected\n",
            "  bitcoin-wallet/bitcoin-wallet: 10/149 files selected\n",
            "  jishenghua/jshERP: 10/263 files selected\n",
            "  JFormDesigner/FlatLaf: 10/332 files selected\n",
            "  jetty/jetty.project: 10/2684 files selected\n",
            "  knowm/XChange: 10/3447 files selected\n",
            "  gh0stkey/HaE: 10/39 files selected\n",
            "  tronprotocol/java-tron: 10/963 files selected\n",
            "  camunda/camunda: 10/9096 files selected\n",
            "  Creators-of-Create/Create: 10/2005 files selected\n",
            "  mercyblitz/tech-weekly: 10/30 files selected\n",
            "  freeplane/freeplane: 10/2073 files selected\n",
            "  google/bundletool: 10/392 files selected\n",
            "  kubernetes-client/java: 10/2856 files selected\n",
            "  Dimezis/BlurView: 10/12 files selected\n",
            "  vipulasri/Timeline-View: 2/2 files selected\n",
            "  shopizer-ecommerce/shopizer: 10/1173 files selected\n",
            "  react-native-share/react-native-share: 10/31 files selected\n",
            "  MovingBlocks/Terasology: 10/2010 files selected\n",
            "  spotbugs/spotbugs: 10/1285 files selected\n",
            "  yacy/yacy_search_server: 10/924 files selected\n",
            "  RoaringBitmap/RoaringBitmap: 10/253 files selected\n",
            "  uncle-novel/uncle-novel: 10/219 files selected\n",
            "  blossom-editor/blossom: 10/399 files selected\n",
            "  zhpanvip/BannerViewPager: 10/17 files selected\n",
            "  tchiotludo/akhq: 10/140 files selected\n",
            "  apache/kylin: 10/2515 files selected\n",
            "  rbmonster/learning-note: 10/441 files selected\n",
            "  undertow-io/undertow: 10/895 files selected\n",
            "  helidon-io/helidon: 10/4425 files selected\n",
            "  jtablesaw/tablesaw: 10/326 files selected\n",
            "  Atmosphere/atmosphere: 10/373 files selected\n",
            "  emanuele-f/PCAPdroid: 10/124 files selected\n",
            "  DataLinkDC/dinky: 10/1322 files selected\n",
            "  OpenHFT/Chronicle-Queue: 10/127 files selected\n",
            "  FasterXML/jackson-databind: 10/572 files selected\n",
            "  networknt/light-4j: 10/544 files selected\n",
            "  springdoc/springdoc-openapi: 10/229 files selected\n",
            "  portfolio-performance/portfolio: 10/1153 files selected\n",
            "  IrisShaders/Iris: 10/721 files selected\n",
            "  1024-lab/smart-admin: 10/1112 files selected\n",
            "  dromara/liteflow: 10/429 files selected\n",
            "  anggrayudi/android-hidden-api: 2/2 files selected\n",
            "  spring-io/initializr: 10/372 files selected\n",
            "  spockframework/spock: 10/589 files selected\n",
            "  fabric8io/kubernetes-client: 10/930 files selected\n",
            "  DependencyTrack/dependency-track: 10/474 files selected\n",
            "  FCL-Team/FoldCraftLauncher: 10/2156 files selected\n",
            "  dtinit/data-transfer-project: 10/638 files selected\n",
            "  TNG/ArchUnit: 10/359 files selected\n",
            "  googlemaps/android-maps-utils: 10/75 files selected\n",
            "  apache/logging-log4j2: 10/1377 files selected\n",
            "  vert-x3/vertx-examples: No Java files found\n",
            "  apache/tika: 10/1322 files selected\n",
            "  immutables/immutables: 10/943 files selected\n",
            "  AxonFramework/AxonFramework: 10/1296 files selected\n",
            "  spring-cloud/spring-cloud-kubernetes: 10/294 files selected\n",
            "  eclipse-openj9/openj9: 10/2042 files selected\n",
            "  NotHarshhaa/DevOps-Projects: 10/26 files selected\n",
            "  apache/incubator-kie-optaplanner: 10/1415 files selected\n",
            "  getActivity/Toaster: 10/23 files selected\n",
            "  M66B/NetGuard: 10/41 files selected\n",
            "  Athou/commafeed: 10/138 files selected\n",
            "  GitLqr/LQRWeChat: 10/231 files selected\n",
            "  apache/linkis: 10/1892 files selected\n",
            "  EngineHub/WorldEdit: 10/1033 files selected\n",
            "  aeron-io/simple-binary-encoding: 10/94 files selected\n",
            "  niumoo/bing-wallpaper: 8/8 files selected\n",
            "  Neamar/KISS: 10/158 files selected\n",
            "  Nekogram/Nekogram: 10/2940 files selected\n",
            "  apache/lucene: 10/3772 files selected\n",
            "  wix/react-native-notifications: 10/31 files selected\n",
            "  dunwu/javacore: 10/750 files selected\n",
            "  jenly1314/ZXingLite: 10/13 files selected\n",
            "  unitycatalog/unitycatalog: 10/145 files selected\n",
            "  openrewrite/rewrite: 10/1765 files selected\n",
            "  prometheus/jmx_exporter: 10/48 files selected\n",
            "  ukanth/afwall: 10/134 files selected\n",
            "  WeBankFinTech/DataSphereStudio: 10/1377 files selected\n",
            "  MeteorDevelopment/meteor-client: 10/957 files selected\n",
            "  spring-projects/spring-data-jpa: 10/232 files selected\n",
            "  AnySoftKeyboard/AnySoftKeyboard: 10/400 files selected\n",
            "  apache/avro: 10/410 files selected\n",
            "  lakesoul-io/LakeSoul: 10/315 files selected\n",
            "  proninyaroslav/libretorrent: 10/283 files selected\n",
            "  florent37/ShapeOfView: 10/13 files selected\n",
            "  qos-ch/logback: 10/596 files selected\n",
            "  xerial/sqlite-jdbc: 10/60 files selected\n",
            "  internetarchive/heritrix3: 10/448 files selected\n",
            "  oracle/visualvm: 10/2217 files selected\n",
            "  apache/paimon: 10/2711 files selected\n",
            "  smartloli/EFAK: 10/209 files selected\n",
            "  apache/curator: 10/495 files selected\n",
            "  wildfly/wildfly: 10/4092 files selected\n",
            "  RaiMan/SikuliX1: 10/275 files selected\n",
            "  Wisser/Jailer: 10/275 files selected\n",
            "  apache/nutch: 10/486 files selected\n",
            "  egzosn/pay-java-parent: 10/238 files selected\n",
            "  CrawlScript/WebCollector: 10/66 files selected\n",
            "  infinilabs/analysis-pinyin: 10/82 files selected\n",
            "  soot-oss/soot: 10/1980 files selected\n",
            "  Minestom/Minestom: 10/1410 files selected\n",
            "  javamelody/javamelody: 10/310 files selected\n",
            "  apache/parquet-java: 10/560 files selected\n",
            "  apache/pdfbox: 10/971 files selected\n",
            "  apache/netbeans: 10/27341 files selected\n",
            "  sleuthkit/autopsy: 10/1796 files selected\n",
            "  FabricMC/fabric-api: 10/1170 files selected\n",
            "  igniterealtime/Openfire: 10/792 files selected\n",
            "  konsoletyper/teavm: 10/3069 files selected\n",
            "  linkedin/cruise-control: 10/486 files selected\n",
            "  itwanger/paicoding: 10/734 files selected\n",
            "  WindySha/Xpatch: 10/49 files selected\n",
            "  synthetichealth/synthea: 10/189 files selected\n",
            "  alldatacenter/alldata: 10/1541 files selected\n",
            "  apache/incubator-hugegraph: 10/1365 files selected\n",
            "  power721/alist-tvbox: 10/421 files selected\n",
            "  spring-projects/spring-data-elasticsearch: 10/399 files selected\n",
            "  classgraph/classgraph: 10/141 files selected\n",
            "  presmihaylov/booknotes: No Java files found\n",
            "  wkeyuan/DWSurvey: 10/428 files selected\n",
            "  algorithmzuo/algorithm-journey: 10/1269 files selected\n",
            "  thymeleaf/thymeleaf: 10/691 files selected\n",
            "  apache/commons-lang: 10/259 files selected\n",
            "  OpenHFT/Chronicle-Map: 10/285 files selected\n",
            "  Tornaco/Thanox: 10/1193 files selected\n",
            "  polymorphicshade/Tubular: 10/321 files selected\n",
            "  spring-projects/spring-batch: 10/800 files selected\n",
            "  mybatis/spring: 10/34 files selected\n",
            "  google/bindiff: 10/1045 files selected\n",
            "  88250/symphony: 10/209 files selected\n",
            "  starcwang/easy_javadoc: 10/75 files selected\n",
            "  jenkinsci/blueocean-plugin: 10/380 files selected\n",
            "  jobrunr/jobrunr: 10/567 files selected\n",
            "  Eanya-Tonic/CCTV_Viewer: 7/7 files selected\n",
            "  apache/gravitino: 10/2356 files selected\n",
            "  WhatsApp/stickers: No Java files found\n",
            "  bytedeco/javacpp-presets: 10/12071 files selected\n",
            "  assertj/assertj: 10/822 files selected\n",
            "  sqlcipher/android-database-sqlcipher: 10/43 files selected\n",
            "  apache/cloudstack: 10/6518 files selected\n",
            "\n",
            "Total Java files selected: 4810\n"
          ]
        }
      ],
      "source": [
        "def find_java_files(repo_path):\n",
        "    \"\"\"\n",
        "    Find all .java files in a repository.\n",
        "    Excludes test files and common non-source directories.\n",
        "    \"\"\"\n",
        "    java_files = []\n",
        "    exclude_patterns = [\"test\", \"tests\", \"example\", \"examples\", \"sample\", \"demo\", \"generated\"]\n",
        "\n",
        "    for root, dirs, files in os.walk(repo_path):\n",
        "        root_lower = root.lower()\n",
        "        if any(pattern in root_lower for pattern in exclude_patterns):\n",
        "            continue\n",
        "\n",
        "        for file in files:\n",
        "            if file.endswith(\".java\"):\n",
        "                java_files.append(os.path.join(root, file))\n",
        "\n",
        "    return java_files\n",
        "\n",
        "\n",
        "def select_java_files(java_files, max_files):\n",
        "    \"\"\"\n",
        "    Randomly select up to max_files from the list.\n",
        "    \"\"\"\n",
        "    if len(java_files) <= max_files:\n",
        "        return java_files\n",
        "    return random.sample(java_files, max_files)\n",
        "\n",
        "\n",
        "# Find and select Java files from each repo\n",
        "repo_java_files = {}\n",
        "all_selected_files = []\n",
        "\n",
        "print(f\"Finding Java files (selecting up to {CLASSES_PER_REPO} per repo)...\\n\")\n",
        "\n",
        "for repo_info in cloned_repos:\n",
        "    repo_name = repo_info[\"repo_name\"]\n",
        "    repo_path = repo_info[\"local_path\"]\n",
        "\n",
        "    java_files = find_java_files(repo_path)\n",
        "\n",
        "    if not java_files:\n",
        "        print(f\"  {repo_name}: No Java files found\")\n",
        "        continue\n",
        "\n",
        "    selected = select_java_files(java_files, max_files=CLASSES_PER_REPO)\n",
        "\n",
        "    repo_java_files[repo_name] = {\n",
        "        \"total_files\": len(java_files),\n",
        "        \"selected_files\": [os.path.relpath(f, repo_path) for f in selected],\n",
        "        \"remaining_files\": len(java_files) - len(selected)\n",
        "    }\n",
        "\n",
        "    all_selected_files.extend([(repo_name, f) for f in selected])\n",
        "    print(f\"  {repo_name}: {len(selected)}/{len(java_files)} files selected\")\n",
        "\n",
        "print(f\"\\nTotal Java files selected: {len(all_selected_files)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dbXJ9L3acyH"
      },
      "source": [
        "---\n",
        "## Parse and Extract Methods\n",
        "\n",
        "Here, we:\n",
        "1. Parse the source code using `javalang`\n",
        "2. Extract all method declarations\n",
        "3. Store the method body along with metadata (repo, file, method name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srlV6DE6acyH",
        "outputId": "48987eca-6a07-44a9-96f7-2f6e2525952b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting methods from 4810 files...\n",
            "\n",
            "  Processed 100/4810 files...\n",
            "  Processed 200/4810 files...\n",
            "  Processed 300/4810 files...\n",
            "  Processed 400/4810 files...\n",
            "  Processed 500/4810 files...\n",
            "  Processed 600/4810 files...\n",
            "  Processed 700/4810 files...\n",
            "  Processed 800/4810 files...\n",
            "  Processed 900/4810 files...\n",
            "  Processed 1000/4810 files...\n",
            "  Processed 1100/4810 files...\n",
            "  Processed 1200/4810 files...\n",
            "  Processed 1300/4810 files...\n",
            "  Processed 1400/4810 files...\n",
            "  Processed 1500/4810 files...\n",
            "  Processed 1600/4810 files...\n",
            "  Processed 1700/4810 files...\n",
            "  Processed 1800/4810 files...\n",
            "  Processed 1900/4810 files...\n",
            "  Processed 2000/4810 files...\n",
            "  Processed 2100/4810 files...\n",
            "  Processed 2200/4810 files...\n",
            "  Processed 2300/4810 files...\n",
            "  Processed 2400/4810 files...\n",
            "  Processed 2500/4810 files...\n",
            "  Processed 2600/4810 files...\n",
            "  Processed 2700/4810 files...\n",
            "  Processed 2800/4810 files...\n",
            "  Processed 2900/4810 files...\n",
            "  Processed 3000/4810 files...\n",
            "  Processed 3100/4810 files...\n",
            "  Processed 3200/4810 files...\n",
            "  Processed 3300/4810 files...\n",
            "  Processed 3400/4810 files...\n",
            "  Processed 3500/4810 files...\n",
            "  Processed 3600/4810 files...\n",
            "  Processed 3700/4810 files...\n",
            "  Processed 3800/4810 files...\n",
            "  Processed 3900/4810 files...\n",
            "  Processed 4000/4810 files...\n",
            "  Processed 4100/4810 files...\n",
            "  Processed 4200/4810 files...\n",
            "  Processed 4300/4810 files...\n",
            "  Processed 4400/4810 files...\n",
            "  Processed 4500/4810 files...\n",
            "  Processed 4600/4810 files...\n",
            "  Processed 4700/4810 files...\n",
            "  Processed 4800/4810 files...\n",
            "\n",
            "Total methods extracted: 39866\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "def read_file_content(file_path):\n",
        "    \"\"\"Read file content with multiple encoding fallbacks and Windows long-path support.\"\"\"\n",
        "    \n",
        "    if os.name == 'nt':\n",
        "        abs_path = os.path.abspath(file_path)\n",
        "        if not abs_path.startswith('\\\\\\\\?\\\\'):\n",
        "            file_path = '\\\\\\\\?\\\\' + abs_path\n",
        "\n",
        "    encodings = ['utf-8', 'latin-1', 'cp1252']\n",
        "\n",
        "    for encoding in encodings:\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding=encoding) as f:\n",
        "                return f.read()\n",
        "        except UnicodeDecodeError:\n",
        "            continue\n",
        "        except FileNotFoundError:\n",
        "            return None\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def extract_method_source(source_code, method_node, lines):\n",
        "    \"\"\"Extract the source code of a method by counting braces.\"\"\"\n",
        "    try:\n",
        "        start_line = method_node.position.line - 1\n",
        "\n",
        "        brace_count = 0\n",
        "        started = False\n",
        "        end_line = start_line\n",
        "\n",
        "        for i in range(start_line, len(lines)):\n",
        "            line = lines[i]\n",
        "            for char in line:\n",
        "                if char == '{':\n",
        "                    brace_count += 1\n",
        "                    started = True\n",
        "                elif char == '}':\n",
        "                    brace_count -= 1\n",
        "\n",
        "            if started and brace_count == 0:\n",
        "                end_line = i\n",
        "                break\n",
        "\n",
        "        method_lines = lines[start_line:end_line + 1]\n",
        "        return '\\n'.join(method_lines)\n",
        "\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "def extract_methods_from_file(file_path, repo_name):\n",
        "    \"\"\"Parse a Java file and extract all methods.\"\"\"\n",
        "    methods = []\n",
        "\n",
        "    source_code = read_file_content(file_path)\n",
        "    if source_code is None:\n",
        "        return methods\n",
        "\n",
        "    lines = source_code.split('\\n')\n",
        "\n",
        "    try:\n",
        "        tree = javalang.parse.parse(source_code)\n",
        "\n",
        "        for path, node in tree.filter(javalang.tree.MethodDeclaration):\n",
        "            method_source = extract_method_source(source_code, node, lines)\n",
        "\n",
        "            if method_source:\n",
        "                methods.append({\n",
        "                    \"repo\": repo_name,\n",
        "                    \"file\": os.path.basename(file_path),\n",
        "                    \"method_name\": node.name,\n",
        "                    \"source\": method_source\n",
        "                })\n",
        "\n",
        "    except javalang.parser.JavaSyntaxError:\n",
        "        pass\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return methods\n",
        "\n",
        "\n",
        "# Extract methods from all selected files\n",
        "all_methods = []\n",
        "\n",
        "print(f\"Extracting methods from {len(all_selected_files)} files...\\n\")\n",
        "\n",
        "for i, (repo_name, file_path) in enumerate(all_selected_files):\n",
        "    if (i + 1) % 100 == 0:\n",
        "        print(f\"  Processed {i + 1}/{len(all_selected_files)} files...\")\n",
        "\n",
        "    methods = extract_methods_from_file(file_path, repo_name)\n",
        "    all_methods.extend(methods)\n",
        "\n",
        "print(f\"\\nTotal methods extracted: {len(all_methods)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8avgVkMTacyH"
      },
      "source": [
        "---\n",
        "## Filter Methods\n",
        "\n",
        "Methods that contain non-ASCII characters and fewer than 10 tokens are removed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxcbkdtaacyH",
        "outputId": "4aac785b-46ef-4b5d-c62d-5145633b05db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtering 39866 methods...\n",
            "\n",
            "Filtering Results:\n",
            "  Total methods:        39866\n",
            "  Dropped (non-ASCII):  1181\n",
            "  Dropped (< 10 tokens): 2186\n",
            "  -------------------------\n",
            "  Methods kept:         36499\n"
          ]
        }
      ],
      "source": [
        "def contains_non_ascii(text):\n",
        "    \"\"\"Check if text contains non-ASCII characters.\"\"\"\n",
        "    try:\n",
        "        text.encode('ascii')\n",
        "        return False\n",
        "    except UnicodeEncodeError:\n",
        "        return True\n",
        "\n",
        "\n",
        "def count_tokens(source_code):\n",
        "    \"\"\"Count the number of Java tokens in source code.\"\"\"\n",
        "    try:\n",
        "        tokens = list(tokenize(source_code))\n",
        "        return len(tokens)\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "\n",
        "# Apply filters\n",
        "filtered_methods = []\n",
        "stats = {\n",
        "    \"total\": len(all_methods),\n",
        "    \"non_ascii_dropped\": 0,\n",
        "    \"too_short_dropped\": 0,\n",
        "    \"kept\": 0\n",
        "}\n",
        "\n",
        "print(f\"Filtering {len(all_methods)} methods...\\n\")\n",
        "\n",
        "for method in all_methods:\n",
        "    source = method[\"source\"]\n",
        "\n",
        "    # 1. Remove non-ASCII characters\n",
        "    if contains_non_ascii(source):\n",
        "        stats[\"non_ascii_dropped\"] += 1\n",
        "        continue\n",
        "\n",
        "    # 2. Filter methods with fewer than MIN_TOKENS (10 tokens)\n",
        "    token_count = count_tokens(source)\n",
        "    if token_count < MIN_TOKENS:\n",
        "        stats[\"too_short_dropped\"] += 1\n",
        "        continue\n",
        "\n",
        "    method[\"token_count\"] = token_count\n",
        "    filtered_methods.append(method)\n",
        "    stats[\"kept\"] += 1\n",
        "\n",
        "print(f\"Filtering Results:\")\n",
        "print(f\"  Total methods:        {stats['total']}\")\n",
        "print(f\"  Dropped (non-ASCII):  {stats['non_ascii_dropped']}\")\n",
        "print(f\"  Dropped (< {MIN_TOKENS} tokens): {stats['too_short_dropped']}\")\n",
        "print(f\"  -------------------------\")\n",
        "print(f\"  Methods kept:         {stats['kept']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YDOr-l0acyH"
      },
      "source": [
        "---\n",
        "## Tokenize Methods\n",
        "\n",
        "Methods are tokenized space-separated tokens using `javalang.tokenizer` such that they look like:\n",
        "\n",
        "```\n",
        "public void setName ( String name ) { this . name = name ; }\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BulBFbsDacyH",
        "outputId": "b5de30ff-2673-4f10-b5fd-2ccd2ff20605"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizing 36499 methods...\n",
            "\n",
            "Successfully tokenized: 36499 methods\n",
            "\n",
            "Example tokenized method:\n",
            "  Repo: iluwatar/java-design-patterns\n",
            "  File: App.java\n",
            "  Method: main\n",
            "  Tokens (70):\n",
            "  public static void main ( String [ ] args ) { var injector = Guice . createInjector ( new LotteryTestingModule ( ) ) ; var administration = injector . getInstance ( LotteryAdministration . class ) ; a...\n"
          ]
        }
      ],
      "source": [
        "def tokenize_method(source_code):\n",
        "    \"\"\"Tokenize Java source code into space-separated tokens.\"\"\"\n",
        "    try:\n",
        "        tokens = list(tokenize(source_code))\n",
        "        token_values = [token.value for token in tokens]\n",
        "        return ' '.join(token_values)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "\n",
        "# Tokenize all methods\n",
        "tokenized_methods = []\n",
        "\n",
        "print(f\"Tokenizing {len(filtered_methods)} methods...\\n\")\n",
        "\n",
        "for method in filtered_methods:\n",
        "    tokenized = tokenize_method(method[\"source\"])\n",
        "\n",
        "    if tokenized:\n",
        "        tokenized_methods.append({\n",
        "            \"repo\": method[\"repo\"],\n",
        "            \"file\": method[\"file\"],\n",
        "            \"method_name\": method[\"method_name\"],\n",
        "            \"tokenized_code\": tokenized,\n",
        "            \"token_count\": method[\"token_count\"]\n",
        "        })\n",
        "\n",
        "print(f\"Successfully tokenized: {len(tokenized_methods)} methods\")\n",
        "\n",
        "print(f\"\\nExample tokenized method:\")\n",
        "if tokenized_methods:\n",
        "    example = tokenized_methods[0]\n",
        "    print(f\"  Repo: {example['repo']}\")\n",
        "    print(f\"  File: {example['file']}\")\n",
        "    print(f\"  Method: {example['method_name']}\")\n",
        "    print(f\"  Tokens ({example['token_count']}):\")\n",
        "    print(f\"  {example['tokenized_code'][:200]}...\" if len(example['tokenized_code']) > 200 else f\"  {example['tokenized_code']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HeN46LCacyH"
      },
      "source": [
        "---\n",
        "## Clean, Deduplicate, and Split\n",
        "\n",
        "Coinduct further removal by finding methods:\n",
        "- With more than one method signatures in one line\n",
        "- That are not complete (not ending with `}`)\n",
        "\n",
        "In addition, duplicate methods are removed and methods are split into train/val/test sets.\n",
        "\n",
        "Finally, vocabs are created from scratch and <UNK> mapping implementation is set up for training stages.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoiqV-RyacyH",
        "outputId": "99f41399-869a-4bae-b8c0-67ee91a92399"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before cleaning: 36499\n",
            "After cleaning: 33942\n",
            "After dedup: 32474\n",
            "\n",
            "Dataset Splits:\n",
            "  T1 (capped at 15000): 15000 methods\n",
            "  T2 (capped at 25000): 25000 methods\n",
            "  T3 (capped at 35000): 30474 methods\n",
            "  Validation: 1000 methods\n",
            "  Test:       1000 methods\n",
            "\n",
            "Vocabularies Built:\n",
            "  Vocab T1 size: 58206 unique tokens\n",
            "  Vocab T2 size: 82923 unique tokens\n",
            "  Vocab T3 size: 93365 unique tokens\n"
          ]
        }
      ],
      "source": [
        "def is_clean_method(tokenized_code):\n",
        "    \"\"\"Check if method is clean (single method, complete).\"\"\"\n",
        "    method_keywords = tokenized_code.count(\"public \") + tokenized_code.count(\"private \") + tokenized_code.count(\"protected \")\n",
        "    if method_keywords > 1:\n",
        "        return False\n",
        "    if not tokenized_code.endswith(\"}\"):\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "# Clean\n",
        "print(f\"Before cleaning: {len(tokenized_methods)}\")\n",
        "tokenized_methods = [m for m in tokenized_methods if is_clean_method(m['tokenized_code'])]\n",
        "print(f\"After cleaning: {len(tokenized_methods)}\")\n",
        "\n",
        "# Deduplicate\n",
        "seen = set()\n",
        "unique_methods = []\n",
        "for m in tokenized_methods:\n",
        "    if m['tokenized_code'] not in seen:\n",
        "        seen.add(m['tokenized_code'])\n",
        "        unique_methods.append(m)\n",
        "\n",
        "print(f\"After dedup: {len(unique_methods)}\")\n",
        "tokenized_methods = unique_methods\n",
        "\n",
        "# Shuffle and split\n",
        "random.shuffle(tokenized_methods)\n",
        "\n",
        "val_size = VAL_SIZE\n",
        "test_size = TEST_SIZE\n",
        "train_size = len(tokenized_methods) - val_size - test_size\n",
        "\n",
        "# 1. Base Splits\n",
        "train_data_full = tokenized_methods[:train_size]\n",
        "val_data = tokenized_methods[train_size:train_size + val_size]\n",
        "test_data = tokenized_methods[train_size + val_size:train_size + val_size + test_size]\n",
        "\n",
        "# 2. Create T1, T2, T3 based on assignment caps\n",
        "T1 = train_data_full[:min(len(train_data_full), T1_CAP)]  # Capped at 15,000\n",
        "T2 = train_data_full[:min(len(train_data_full), T2_CAP)]  # Capped at 25,000\n",
        "T3 = train_data_full[:min(len(train_data_full), T3_CAP)]  # Capped at 35,000\n",
        "\n",
        "print(f\"\\nDataset Splits:\")\n",
        "print(f\"  T1 (capped at {T1_CAP}): {len(T1)} methods\")\n",
        "print(f\"  T2 (capped at {T2_CAP}): {len(T2)} methods\")\n",
        "print(f\"  T3 (capped at {T3_CAP}): {len(T3)} methods\")\n",
        "print(f\"  Validation: {len(val_data)} methods\")\n",
        "print(f\"  Test:       {len(test_data)} methods\")\n",
        "\n",
        "# 3. Build separate vocabularies from scratch for T1, T2, T3\n",
        "def build_vocab(dataset):\n",
        "    \"\"\"Builds a vocabulary set of unique tokens from a given dataset.\"\"\"\n",
        "    vocab = set()\n",
        "    for m in dataset:\n",
        "        tokens = m['tokenized_code'].split()\n",
        "        vocab.update(tokens)\n",
        "    return vocab\n",
        "\n",
        "vocab_T1 = build_vocab(T1)\n",
        "vocab_T2 = build_vocab(T2)\n",
        "vocab_T3 = build_vocab(T3)\n",
        "\n",
        "print(f\"\\nVocabularies Built:\")\n",
        "print(f\"  Vocab T1 size: {len(vocab_T1)} unique tokens\")\n",
        "print(f\"  Vocab T2 size: {len(vocab_T2)} unique tokens\")\n",
        "print(f\"  Vocab T3 size: {len(vocab_T3)} unique tokens\")\n",
        "\n",
        "# 4. Implement <UNK> Mapping for Validation/Testing\n",
        "def apply_unk_mapping(dataset, vocab):\n",
        "    \"\"\"Maps tokens not present in the corresponding vocabulary to <UNK>.\"\"\"\n",
        "    mapped_dataset = []\n",
        "    for m in dataset:\n",
        "        tokens = m['tokenized_code'].split()\n",
        "        # If token is in vocab, keep it. Otherwise, replace with <UNK>\n",
        "        mapped_tokens = [t if t in vocab else \"<UNK>\" for t in tokens]\n",
        "        \n",
        "        # Create a copy so we don't mutate the original dictionary permanently\n",
        "        new_m = m.copy()\n",
        "        new_m['tokenized_code'] = ' '.join(mapped_tokens)\n",
        "        mapped_dataset.append(new_m)\n",
        "        \n",
        "    return mapped_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6PGNPWracyI"
      },
      "source": [
        "---\n",
        "## Save Outputs\n",
        "\n",
        "We save:\n",
        "1. `train.txt` - One tokenized method per line\n",
        "2. `val.txt` - Validation set\n",
        "3. `test.txt` - Test set\n",
        "4. `metadata.json` - Tracks which repos/files were used (for students)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zIS0mR6acyI",
        "outputId": "a2cab7c3-41d6-45ce-d295-9de151d77045"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving dataset files...\n",
            "\n",
            "  Saved: dataset\\ngram_dataset\\train_T1.txt\n",
            "  Saved: dataset\\ngram_dataset\\train_T2.txt\n",
            "  Saved: dataset\\ngram_dataset\\train_T3.txt\n",
            "  Saved: dataset\\ngram_dataset\\val.txt\n",
            "  Saved: dataset\\ngram_dataset\\test.txt\n",
            "  Saved: dataset\\ngram_dataset\\metadata.json\n",
            "\n",
            "All files saved to: dataset\\ngram_dataset/\n"
          ]
        }
      ],
      "source": [
        "def save_txt(data, filename):\n",
        "    \"\"\"Save tokenized methods to a text file (one per line).\"\"\"\n",
        "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
        "    with open(filepath, 'w', encoding='utf-8', errors='replace') as f:\n",
        "        for method in data:\n",
        "            f.write(method['tokenized_code'] + '\\n')\n",
        "    return filepath\n",
        "\n",
        "# Save train/val/test files\n",
        "print(\"Saving dataset files...\\n\")\n",
        "\n",
        "# Save the three distinct training sets\n",
        "train_t1_path = save_txt(T1, \"train_T1.txt\")\n",
        "print(f\"  Saved: {train_t1_path}\")\n",
        "\n",
        "train_t2_path = save_txt(T2, \"train_T2.txt\")\n",
        "print(f\"  Saved: {train_t2_path}\")\n",
        "\n",
        "train_t3_path = save_txt(T3, \"train_T3.txt\")\n",
        "print(f\"  Saved: {train_t3_path}\")\n",
        "\n",
        "# Save validation and test sets\n",
        "val_path = save_txt(val_data, \"val.txt\")\n",
        "print(f\"  Saved: {val_path}\")\n",
        "\n",
        "test_path = save_txt(test_data, \"test.txt\")\n",
        "print(f\"  Saved: {test_path}\")\n",
        "\n",
        "# Save metadata\n",
        "metadata = {\n",
        "    \"description\": \"Metadata for N-gram dataset. Tracks files used and exact split sizes.\",\n",
        "    \"instructions_for_self_created_test_set\": [\n",
        "        \"To create your self-created test set with ~1K methods:\",\n",
        "        \"  1. Use the same repositories listed below\",\n",
        "        \"  2. Select Java files NOT in 'selected_files'\",\n",
        "        \"  3. Extract methods using the exact same pipeline\"\n",
        "    ],\n",
        "    \"dataset_stats\": {\n",
        "        \"T1_size\": len(T1),\n",
        "        \"T2_size\": len(T2),\n",
        "        \"T3_size\": len(T3),\n",
        "        \"val_size\": len(val_data),\n",
        "        \"test_size\": len(test_data),\n",
        "        \"total_repos\": len(repo_java_files),\n",
        "        \"min_tokens\": MIN_TOKENS\n",
        "    },\n",
        "    \"repos_used\": repo_java_files\n",
        "}\n",
        "\n",
        "metadata_path = os.path.join(OUTPUT_DIR, \"metadata.json\")\n",
        "with open(metadata_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "print(f\"  Saved: {metadata_path}\")\n",
        "\n",
        "print(f\"\\nAll files saved to: {OUTPUT_DIR}/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjFya3wlacyI"
      },
      "source": [
        "---\n",
        "## Summary Statistics for data pipeline and cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSyyQW8YacyI",
        "outputId": "a37b91ee-d50f-46c9-828e-e2c1d233136e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "         DATASET CREATION SUMMARY\n",
            "==================================================\n",
            "\n",
            "Repositories:\n",
            "   Cloned:    499\n",
            "   Failed:    1\n",
            "\n",
            "Java Files:\n",
            "   Selected:  4810\n",
            "\n",
            "Methods:\n",
            "   Extracted: 39866\n",
            "   Filtered:  7392 removed\n",
            "   Final:     32474\n",
            "\n",
            "Token Statistics:\n",
            "   Min:    10\n",
            "   Max:    33291\n",
            "   Mean:   60.9\n",
            "   Median: 27.0\n",
            "\n",
            "Dataset Splits:\n",
            "   T1 Train:   15,000 methods\n",
            "   T2 Train:   25,000 methods\n",
            "   T3 Train:   30,474 methods\n",
            "   Validation: 1,000 methods\n",
            "   Test:       1,000 methods\n",
            "\n",
            "Output Files:\n",
            "   dataset\\ngram_dataset/train_T1.txt\n",
            "   dataset\\ngram_dataset/train_T2.txt\n",
            "   dataset\\ngram_dataset/train_T3.txt\n",
            "   dataset\\ngram_dataset/val.txt\n",
            "   dataset\\ngram_dataset/test.txt\n",
            "   dataset\\ngram_dataset/metadata.json\n",
            "\n",
            "==================================================\n",
            "         DATASET READY FOR N-GRAM TRAINING\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "import statistics\n",
        "\n",
        "all_token_counts = [m['token_count'] for m in tokenized_methods]\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"         DATASET CREATION SUMMARY\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(f\"\\nRepositories:\")\n",
        "print(f\"   Cloned:    {len(cloned_repos)}\")\n",
        "print(f\"   Failed:    {len(failed_repos)}\")\n",
        "\n",
        "print(f\"\\nJava Files:\")\n",
        "print(f\"   Selected:  {len(all_selected_files)}\")\n",
        "\n",
        "print(f\"\\nMethods:\")\n",
        "print(f\"   Extracted: {stats['total']}\")\n",
        "print(f\"   Filtered:  {stats['total'] - len(tokenized_methods)} removed\")\n",
        "print(f\"   Final:     {len(tokenized_methods)}\")\n",
        "\n",
        "print(f\"\\nToken Statistics:\")\n",
        "print(f\"   Min:    {min(all_token_counts)}\")\n",
        "print(f\"   Max:    {max(all_token_counts)}\")\n",
        "print(f\"   Mean:   {statistics.mean(all_token_counts):.1f}\")\n",
        "print(f\"   Median: {statistics.median(all_token_counts):.1f}\")\n",
        "\n",
        "print(f\"\\nDataset Splits:\")\n",
        "print(f\"   T1 Train:   {len(T1):,} methods\")\n",
        "print(f\"   T2 Train:   {len(T2):,} methods\")\n",
        "print(f\"   T3 Train:   {len(T3):,} methods\")\n",
        "print(f\"   Validation: {len(val_data):,} methods\")\n",
        "print(f\"   Test:       {len(test_data):,} methods\")\n",
        "\n",
        "print(f\"\\nOutput Files:\")\n",
        "print(f\"   {OUTPUT_DIR}/train_T1.txt\")\n",
        "print(f\"   {OUTPUT_DIR}/train_T2.txt\")\n",
        "print(f\"   {OUTPUT_DIR}/train_T3.txt\")\n",
        "print(f\"   {OUTPUT_DIR}/val.txt\")\n",
        "print(f\"   {OUTPUT_DIR}/test.txt\")\n",
        "print(f\"   {OUTPUT_DIR}/metadata.json\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"         DATASET READY FOR N-GRAM TRAINING\")\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# N-gram Language Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import json\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "class NGramLanguageModel:\n",
        "    def __init__(self, n, vocab, alpha=0.01):\n",
        "        \"\"\"\n",
        "        Initializes the N-gram model.\n",
        "        :param n: The context window size (e.g., 3, 5, or 7).\n",
        "        :param vocab: The vocabulary set for the training data.\n",
        "        :param alpha: Smoothing parameter for Add-alpha smoothing.\n",
        "        \"\"\"\n",
        "        self.n = n\n",
        "        self.alpha = alpha\n",
        "        \n",
        "        # Ensure <UNK> is in the vocabulary\n",
        "        self.vocab = set(vocab)\n",
        "        self.vocab.add(\"<UNK>\")\n",
        "        self.vocab_size = len(self.vocab)\n",
        "        \n",
        "        # Data structures to store counts\n",
        "        self.context_counts = defaultdict(Counter)\n",
        "        self.context_totals = Counter()\n",
        "        self.unigram_counts = Counter()\n",
        "        \n",
        "    def train(self, dataset):\n",
        "        \"\"\"Trains the model by calculating n-gram and context counts.\"\"\"\n",
        "        for method in dataset:\n",
        "            tokens = method['tokenized_code'].split()\n",
        "            \n",
        "            # Count unigrams for fallback predictions\n",
        "            for t in tokens:\n",
        "                self.unigram_counts[t] += 1\n",
        "                \n",
        "            # Pad the beginning of the sequence to handle initial context\n",
        "            padded_tokens = [\"<s>\"] * (self.n - 1) + tokens\n",
        "            \n",
        "            for i in range(len(padded_tokens) - self.n + 1):\n",
        "                context = tuple(padded_tokens[i : i + self.n - 1])\n",
        "                target = padded_tokens[i + self.n - 1]\n",
        "                \n",
        "                self.context_counts[context][target] += 1\n",
        "                self.context_totals[context] += 1\n",
        "\n",
        "    def get_probability(self, context, target):\n",
        "        \"\"\"\n",
        "        Computes the smoothed probability P(target | context).\n",
        "        \"\"\"\n",
        "        count_ngram = self.context_counts[context][target]\n",
        "        count_context = self.context_totals[context]\n",
        "        \n",
        "        # Add-alpha smoothing formula\n",
        "        prob = (count_ngram + self.alpha) / (count_context + self.alpha * self.vocab_size)\n",
        "        return prob\n",
        "\n",
        "    def predict_argmax(self, context):\n",
        "        \"\"\"\n",
        "        Returns the most probable next token and its probability.\n",
        "        \"\"\"\n",
        "        if context in self.context_counts and self.context_totals[context] > 0:\n",
        "            best_token = self.context_counts[context].most_common(1)[0][0]\n",
        "            best_prob = self.get_probability(context, best_token)\n",
        "            return best_token, best_prob\n",
        "        else:\n",
        "            # Backoff: If context is entirely unseen, predict the most common unigram\n",
        "            best_token = self.unigram_counts.most_common(1)[0][0]\n",
        "            best_prob = self.get_probability(context, best_token)\n",
        "            return best_token, best_prob\n",
        "\n",
        "    def compute_perplexity(self, dataset):\n",
        "        \"\"\"\n",
        "        Computes perplexity over a dataset using ground-truth probabilities.\n",
        "        \"\"\"\n",
        "        log_prob_sum = 0.0\n",
        "        N = 0\n",
        "        \n",
        "        for method in dataset:\n",
        "            tokens = method['tokenized_code'].split()\n",
        "            padded_tokens = [\"<s>\"] * (self.n - 1) + tokens\n",
        "            \n",
        "            for i in range(len(padded_tokens) - self.n + 1):\n",
        "                context = tuple(padded_tokens[i : i + self.n - 1])\n",
        "                target = padded_tokens[i + self.n - 1]\n",
        "                \n",
        "                # We use the probability of the ground-truth token, NOT the argmax prediction\n",
        "                prob = self.get_probability(context, target)\n",
        "                log_prob_sum += math.log(prob)\n",
        "                N += 1\n",
        "                \n",
        "        if N == 0: return float('inf')\n",
        "        \n",
        "        return math.exp(-log_prob_sum / N)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training and Validating Models across T1, T2, and T3...\n",
            "============================================================\n",
            "\n",
            "--- Evaluating Training Set: T1 ---\n",
            "Training 3-gram model on T1...\n",
            "  -> Perplexity for T1 (n=3): 1451.24\n",
            "Training 5-gram model on T1...\n",
            "  -> Perplexity for T1 (n=5): 10033.38\n",
            "Training 7-gram model on T1...\n",
            "  -> Perplexity for T1 (n=7): 21076.88\n",
            "\n",
            "--- Evaluating Training Set: T2 ---\n",
            "Training 3-gram model on T2...\n",
            "  -> Perplexity for T2 (n=3): 1341.57\n",
            "Training 5-gram model on T2...\n",
            "  -> Perplexity for T2 (n=5): 10302.00\n",
            "Training 7-gram model on T2...\n",
            "  -> Perplexity for T2 (n=7): 23986.11\n",
            "\n",
            "--- Evaluating Training Set: T3 ---\n",
            "Training 3-gram model on T3...\n",
            "  -> Perplexity for T3 (n=3): 1293.73\n",
            "Training 5-gram model on T3...\n",
            "  -> Perplexity for T3 (n=5): 10259.14\n",
            "Training 7-gram model on T3...\n",
            "  -> Perplexity for T3 (n=7): 24700.76\n",
            "\n",
            "============================================================\n",
            "Best Configuration: Dataset = T3, n = 3\n",
            "Lowest Validation Perplexity: 1293.73\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Group the training sets and their corresponding vocabularies\n",
        "training_sets = {\n",
        "    \"T1\": (T1, vocab_T1),\n",
        "    \"T2\": (T2, vocab_T2),\n",
        "    \"T3\": (T3, vocab_T3)\n",
        "}\n",
        "\n",
        "n_values = [3, 5, 7]\n",
        "models = {}\n",
        "\n",
        "best_config = None\n",
        "best_perplexity = float('inf')\n",
        "best_model = None\n",
        "best_vocab = None\n",
        "\n",
        "print(\"Training and Validating Models across T1, T2, and T3...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for set_name, (train_data, vocab) in training_sets.items():\n",
        "    print(f\"\\n--- Evaluating Training Set: {set_name} ---\")\n",
        "    \n",
        "    # Map unseen validation tokens to <UNK> using the specific vocabulary for this training set\n",
        "    val_data_mapped = apply_unk_mapping(val_data, vocab)\n",
        "    \n",
        "    for n in n_values:\n",
        "        print(f\"Training {n}-gram model on {set_name}...\")\n",
        "        model = NGramLanguageModel(n=n, vocab=vocab, alpha=0.1)\n",
        "        model.train(train_data)\n",
        "        \n",
        "        # Evaluate on the mapped validation set\n",
        "        perplexity = model.compute_perplexity(val_data_mapped)\n",
        "        print(f\"  -> Perplexity for {set_name} (n={n}): {perplexity:.2f}\")\n",
        "        \n",
        "        # Store the model using a tuple key (Dataset, n)\n",
        "        models[(set_name, n)] = model\n",
        "        \n",
        "        # Keep track of the absolute best configuration\n",
        "        if perplexity < best_perplexity:\n",
        "            best_perplexity = perplexity\n",
        "            best_config = (set_name, n)\n",
        "            best_model = model\n",
        "            best_vocab = vocab\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(f\"Best Configuration: Dataset = {best_config[0]}, n = {best_config[1]}\")\n",
        "print(f\"Lowest Validation Perplexity: {best_perplexity:.2f}\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating predictions for test.txt...\n",
            "  -> Saved dataset\\ngram_dataset\\results-self-test.json (Perplexity: 1210.13)\n",
            "Generating predictions for given_test.txt...\n",
            "  -> Saved dataset\\ngram_dataset\\results-given-test.json (Perplexity: 1864.57)\n"
          ]
        }
      ],
      "source": [
        "def generate_json_results(model, dataset, test_set_name, output_filename):\n",
        "    \"\"\"\n",
        "    Evaluates the model on a test set and exports the detailed JSON structure.\n",
        "    \"\"\"\n",
        "    # Compute overall perplexity first using the ground-truth probabilities\n",
        "    overall_perplexity = model.compute_perplexity(dataset)\n",
        "    \n",
        "    results = {\n",
        "        \"testSet\": test_set_name,\n",
        "        \"perplexity\": round(overall_perplexity, 2),\n",
        "        \"data\": []\n",
        "    }\n",
        "    \n",
        "    print(f\"Generating predictions for {test_set_name}...\")\n",
        "    \n",
        "    for idx, method in enumerate(dataset):\n",
        "        tokens = method['tokenized_code'].split()\n",
        "        padded_tokens = [\"<s>\"] * (model.n - 1) + tokens\n",
        "        \n",
        "        predictions_list = []\n",
        "        \n",
        "        for i in range(len(padded_tokens) - model.n + 1):\n",
        "            context = tuple(padded_tokens[i : i + model.n - 1])\n",
        "            ground_truth = padded_tokens[i + model.n - 1]\n",
        "            \n",
        "            # For JSON output, predProbability is the max probability of the model's chosen token\n",
        "            pred_token, pred_prob = model.predict_argmax(context)\n",
        "            \n",
        "            # Clean up padding tokens for the JSON output context array\n",
        "            clean_context = [c for c in context if c != \"<s>\"]\n",
        "            \n",
        "            predictions_list.append({\n",
        "                \"context\": clean_context,\n",
        "                \"predToken\": pred_token,\n",
        "                \"predProbability\": round(pred_prob, 4),\n",
        "                \"groundTruth\": ground_truth\n",
        "            })\n",
        "            \n",
        "        results[\"data\"].append({\n",
        "            \"index\": f\"ID{idx + 1}\",\n",
        "            \"tokenizedCode\": method['tokenized_code'],\n",
        "            \"contextWindow\": model.n,\n",
        "            \"predictions\": predictions_list\n",
        "        })\n",
        "        \n",
        "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "        \n",
        "    print(f\"  -> Saved {output_filename} (Perplexity: {overall_perplexity:.2f})\")\n",
        "\n",
        "\n",
        "def load_external_test_set(filepath):\n",
        "    \"\"\"Loads a tokenized test set from a text file into the dataset format.\"\"\"\n",
        "    dataset = []\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line:  # Skip empty lines\n",
        "                dataset.append({'tokenized_code': line})\n",
        "    return dataset\n",
        "\n",
        "\n",
        "test_path = os.path.join(OUTPUT_DIR, \"test.txt\")\n",
        "extract_test_data = load_external_test_set(test_path)\n",
        "output_path = os.path.join(OUTPUT_DIR, \"results-self-test.json\")\n",
        "\n",
        "test_data_mapped = apply_unk_mapping(extract_test_data, best_vocab)\n",
        "generate_json_results(\n",
        "    model=best_model, \n",
        "    dataset=test_data_mapped, \n",
        "    test_set_name=\"test.txt\", \n",
        "    output_filename=output_path\n",
        ")\n",
        "\n",
        "\n",
        "given_test_path = os.path.join(OUTPUT_DIR, \"given_test.txt\")\n",
        "extract_test_data = load_external_test_set(given_test_path)\n",
        "output_path = os.path.join(OUTPUT_DIR, \"results-given-test.json\")\n",
        "\n",
        "provided_test_mapped = apply_unk_mapping(extract_test_data, best_vocab)\n",
        "generate_json_results(\n",
        "    model=best_model, \n",
        "    dataset=provided_test_mapped, \n",
        "    test_set_name=\"given_test.txt\", \n",
        "    output_filename=output_path\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
